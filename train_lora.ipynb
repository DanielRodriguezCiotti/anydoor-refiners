{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "torch.set_num_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.data.vitonhd import VitonHDDataset,CustomDataLoader\n",
    "from src.training.data.batch import collate_fn\n",
    "\n",
    "dataset = VitonHDDataset(\"dataset/train/cloth\", filtering_file=\"dataset/lora_training_images.txt\")\n",
    "dataloader = CustomDataLoader(dataset, batch_size=5, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.anydoor_refiners.model import AnyDoor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32\n",
    "\n",
    "anydoor = AnyDoor(num_inference_steps = 1000, use_tv_loss=False,device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "object = batch.object.to(device, dtype)\n",
    "background = batch.background.to(device, dtype)\n",
    "collage = batch.collage.to(device, dtype)\n",
    "batch_size = object.shape[0]\n",
    "loss_mask = batch.loss_mask.to(device, dtype)\n",
    "timestep = random.randint(0,999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_sample( images: torch.Tensor,noise: torch.Tensor, timestep : int):\n",
    "    scale_factor = anydoor.solver.cumulative_scale_factors[timestep]\n",
    "    sqrt_one_minus_scale_factor =anydoor.solver.noise_std[timestep]\n",
    "    return scale_factor * images + sqrt_one_minus_scale_factor * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    object_embedding = anydoor.object_encoder.forward(object)\n",
    "    noise = anydoor.sample_noise(size=(batch_size, 4, 64, 64), device=device, dtype=dtype)\n",
    "    background_latents = anydoor.lda.encode(background)\n",
    "    noisy_backgrounds = q_sample(background_latents, noise, timestep)\n",
    "    predicted_noise = anydoor.forward(noisy_backgrounds,step=timestep,control_background_image=collage,object_embedding=object_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shapes': []}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "anydoor.unet.use_context(\"sampling\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ones in mask:  tensor(4347., device='cuda:0')\n",
      "Number of zeros in mask:  tensor(16133., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "## Print number of ones and zeros in the mask\n",
    "print(\"Number of ones in mask: \", torch.sum(loss_mask))\n",
    "print(\"Number of zeros in mask: \", torch.sum(1-loss_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = TrainingConfig(\n",
    "    duration=Epoch(10),\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "optimizer = OptimizerConfig(\n",
    "    optimizer=Optimizers.AdamW,\n",
    "    learning_rate=1e-5,\n",
    ")\n",
    "\n",
    "lr_scheduler = LRSchedulerConfig(\n",
    "    type=LRSchedulerType.CONSTANT_LR,\n",
    ")\n",
    "\n",
    "anydoor_config = AnydoorModelConfig(\n",
    "    path_to_unet=\"ckpt/refiners/unet.safetensors\",\n",
    "    path_to_control_model=\"ckpt/refiners/controlnet.safetensors\",\n",
    "    path_to_object_encoder=\"ckpt/refiners/dinov2_encoder.safetensors\",\n",
    "    path_to_lda=\"ckpt/refiners/lda_new.safetensors\",\n",
    ")\n",
    "\n",
    "training_config = AnydoorTrainingConfig(\n",
    "    train_dataset='dataset/train/cloth',\n",
    "    test_dataset='dataset/test/cloth',\n",
    "    batch_size=16,\n",
    "    anydoor=anydoor_config,\n",
    "    training=training,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 11:11:52.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.trainer\u001b[0m:\u001b[36mdevice\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUsing device: cuda\u001b[0m\n",
      "\u001b[32m2024-11-20 11:11:52.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.trainer\u001b[0m:\u001b[36mdtype\u001b[0m:\u001b[36m160\u001b[0m - \u001b[1mUsing dtype: torch.float32\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 11:12:40.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.trainer\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mNumber of learnable parameters in anydoor: 6.9M\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer = AnyDoorLoRATrainer(training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnable parameters: 6.9M\n",
      "Total parameters: 2.5G\n",
      "Percetage of learnable parameters: 0.28%\n"
     ]
    }
   ],
   "source": [
    "# Count number of learnable parameters\n",
    "import numpy as np\n",
    "\n",
    "def human_readable_number(number) -> str:\n",
    "    float_number = float(number)\n",
    "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
    "        if abs(float_number) < 1000:\n",
    "            return f\"{float_number:.1f}{unit}\"\n",
    "        float_number /= 1000\n",
    "    return f\"{float_number:.1f}E\"\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, trainer.anydoor.parameters())\n",
    "learnable_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "total_params = sum(p.numel() for p in trainer.anydoor.parameters())\n",
    "print(f\"Learnable parameters: {human_readable_number(learnable_params)}\")\n",
    "print(f\"Total parameters: {human_readable_number(total_params)}\")\n",
    "print(f\"Percetage of learnable parameters: {learnable_params / total_params * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 11:12:40.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mStarting training for Epoch(number=100).\u001b[0m\n",
      "\u001b[32m2024-11-20 11:12:40.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mEpoch 0 started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 11:12:41.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mIteration 0 started.\u001b[0m\n",
      "\u001b[32m2024-11-20 11:12:41.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mStep 0 started.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.07028716802597046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 11:12:44.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.trainer\u001b[0m:\u001b[36moptimizer\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mTotal number of learnable parameters in the model(s): 6.9M\u001b[0m\n",
      "/home/daniel/code/anydoor-refiners/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-11-20 11:12:44.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mStep 0 ended.\u001b[0m\n",
      "\u001b[32m2024-11-20 11:12:46.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mIteration 0 ended.\u001b[0m\n",
      "\u001b[32m2024-11-20 11:12:46.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mIteration 1 started.\u001b[0m\n",
      "\u001b[32m2024-11-20 11:12:46.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mStep 1 started.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08815356343984604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 11:12:48.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mStep 1 ended.\u001b[0m\n",
      "\u001b[32m2024-11-20 11:12:49.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mIteration 1 ended.\u001b[0m\n",
      "\u001b[32m2024-11-20 11:12:49.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mIteration 2 started.\u001b[0m\n",
      "\u001b[32m2024-11-20 11:12:49.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mStep 2 started.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.006110812537372112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 11:12:51.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mStep 2 ended.\u001b[0m\n",
      "\u001b[32m2024-11-20 11:12:53.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mIteration 2 ended.\u001b[0m\n",
      "\u001b[32m2024-11-20 11:12:53.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mIteration 3 started.\u001b[0m\n",
      "\u001b[32m2024-11-20 11:12:53.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mStep 3 started.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08576411008834839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 11:12:55.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrefiners.training_utils.clock\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mStep 3 ended.\u001b[0m\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Print number of total parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/anydoor-refiners/.venv/lib/python3.11/site-packages/refiners/training_utils/common.py:61\u001b[0m, in \u001b[0;36mscoped_seed.__call__.<locals>.inner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactual_seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/anydoor-refiners/.venv/lib/python3.11/site-packages/refiners/training_utils/trainer.py:336\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callbacks(event_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_epoch_begin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callbacks(event_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callbacks(event_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/code/anydoor-refiners/.venv/lib/python3.11/site-packages/refiners/training_utils/trainer.py:317\u001b[0m, in \u001b[0;36mTrainer.epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mepoch\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform a single epoch.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "File \u001b[0;32m~/code/anydoor-refiners/src/training/trainer.py:86\u001b[0m, in \u001b[0;36mAnyDoorLoRATrainer.create_data_iterable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Use a generator to yield transformed items\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mAnydoorBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/anydoor-refiners/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/code/anydoor-refiners/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/code/anydoor-refiners/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/code/anydoor-refiners/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/code/anydoor-refiners/src/training/data/base.py:59\u001b[0m, in \u001b[0;36mBaseDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     58\u001b[0m     idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[0;32m~/code/anydoor-refiners/src/training/data/vitonhd.py:53\u001b[0m, in \u001b[0;36mVitonHDDataset.get_sample\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     50\u001b[0m tar_mask\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(tar_mask)\n\u001b[1;32m     51\u001b[0m tar_mask \u001b[38;5;241m=\u001b[39m tar_mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m---> 53\u001b[0m item_with_collage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtar_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtar_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m sampled_time_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_timestep()\n\u001b[1;32m     55\u001b[0m item_with_collage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_steps\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sampled_time_steps\n",
      "File \u001b[0;32m~/code/anydoor-refiners/src/training/data/base.py:87\u001b[0m, in \u001b[0;36mBaseDataset.process_pairs\u001b[0;34m(self, ref_image, ref_mask, tar_image, tar_mask, max_ratio)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_pairs\u001b[39m(\u001b[38;5;28mself\u001b[39m, ref_image, ref_mask, tar_image, tar_mask, max_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m):\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m mask_score(ref_mask) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.90\u001b[39m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_mask_area(ref_mask) \n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_mask_area(tar_mask) \n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Print number of total parameters\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
