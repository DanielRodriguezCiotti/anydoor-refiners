{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from refiners.fluxion.adapters.lora import LinearLora, LoraAdapter\n",
    "from anydoor_refiners.model import AnyDoor\n",
    "import refiners.fluxion.layers as fl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lora(model : AnyDoor):\n",
    "    for dinov2_cross_attn_block in model.unet.layers(fl.Residual):\n",
    "        if dinov2_cross_attn_block._get_name() == \"DinoV2CrossAttention\":\n",
    "            dinov2_cross_attn_block_layers = {}\n",
    "            dinov2_cross_attn_block_layers[\"proj_in\"] = {'layer' : dinov2_cross_attn_block.layer(0,fl.Chain).layer(-1,fl.Linear), 'parent' : dinov2_cross_attn_block.layer(0,fl.Chain)}\n",
    "            dinov2_cross_attn_block_layers[\"proj_out\"] = {'layer' :dinov2_cross_attn_block.layer(-1,fl.Chain).layer(0,fl.Linear), 'parent' : dinov2_cross_attn_block.layer(-1,fl.Chain)}\n",
    "            prefix_att = [\"q\",\"k\",\"v\"]\n",
    "            for i,attention in enumerate(dinov2_cross_attn_block.layers(fl.Attention)):\n",
    "                for j,linear in enumerate(attention.layer(-3,fl.Distribute).layers(fl.Linear)):\n",
    "                    dinov2_cross_attn_block_layers[f\"attn_{i}_{prefix_att[j]}\"] = {'layer' : linear, 'parent' : attention.layer(-3,fl.Distribute)}\n",
    "                dinov2_cross_attn_block_layers[f\"attn_{i}_out\"] = {'layer' :attention.layer(-1,fl.Linear), 'parent' : attention}\n",
    "            dinov2_cross_attn_block_layers[\"ffn_1\"] ={'layer' : dinov2_cross_attn_block.layer(-2,fl.Chain).layer(-1,fl.Chain).layer(-1,fl.Residual).layer(1,fl.Linear), 'parent' : dinov2_cross_attn_block.layer(-2,fl.Chain).layer(-1,fl.Chain).layer(-1,fl.Residual)}\n",
    "            dinov2_cross_attn_block_layers[\"ffn_2\"] = {'layer' :dinov2_cross_attn_block.layer(-2,fl.Chain).layer(-1,fl.Chain).layer(-1,fl.Residual).layer(-1,fl.Linear), 'parent' : dinov2_cross_attn_block.layer(-2,fl.Chain).layer(-1,fl.Chain).layer(-1,fl.Residual)}\n",
    "                \n",
    "            for key,layer_dict in dinov2_cross_attn_block_layers.items():\n",
    "                adapter = LoraAdapter(layer_dict['layer'],LinearLora(key,layer_dict['layer'].in_features,layer_dict['layer'].out_features))\n",
    "                adapter.inject(layer_dict['parent'])\n",
    "\n",
    "def get_lora_weights(base: fl.Chain) -> dict[str, torch.Tensor]:\n",
    "    prev_parent: fl.Chain | None = None\n",
    "    lora_weights: dict[str, torch.Tensor] = {}\n",
    "    n = 0\n",
    "    for lora_adapter, parent in base.walk(LoraAdapter):\n",
    "        for lora in lora_adapter.lora_layers :\n",
    "        # lora = next((l for l in lora_adapter.lora_layers if l.name == name), None)\n",
    "            if lora is None:\n",
    "                continue\n",
    "            n = (parent == prev_parent) and n + 1 or 1\n",
    "            pfx = f\"{parent.get_path()}.{n}.{lora_adapter.target.__class__.__name__}\"\n",
    "            lora_weights[f\"{pfx}.down.weight\"] = lora.down.weight\n",
    "            lora_weights[f\"{pfx}.up.weight\"] = lora.up.weight\n",
    "            prev_parent = parent\n",
    "    return lora_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anydoor = AnyDoor()\n",
    "build_lora(anydoor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in anydoor.named_parameters():\n",
    "    if \"LinearLora\" in name:\n",
    "        param.requires_grad = True  # Leave these layers trainable\n",
    "    else:\n",
    "        param.requires_grad = False  # Freeze other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
