{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "import sys\n",
    "sys.path.append(\"./AnyDoor/\")\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_dict_from_safetensors(path:str):\n",
    "    \"\"\" Load a state dict from a safetensors file \"\"\"\n",
    "    tensors = {}\n",
    "    with safe_open(path, framework=\"pt\", device=device) as f:\n",
    "        for key in f.keys():\n",
    "            tensors[key] = f.get_tensor(key)\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(\"./tests/tensors/x.pt\",weights_only=True)\n",
    "initial_latents = torch.randn(1,4,32,32)\n",
    "object_embedding = torch.load(\"./tests/tensors/object_embedding.pt\",weights_only=True)\n",
    "negative_object_embedding = torch.load(\"./tests/tensors/negative_object_embedding.pt\",weights_only=True)\n",
    "control = torch.load(\"./tests/tensors/control_features.pt\",weights_only=True)\n",
    "inference_steps = 10\n",
    "scale = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anydoor_original.cldm import model as cldm\n",
    "from cldm.ddim_hacked import DDIMSampler\n",
    "sampler = DDIMSampler(cldm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 10 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  10%|█         | 1/10 [00:01<00:17,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(68.5324)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  20%|██        | 2/10 [00:03<00:15,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(71.9554)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  30%|███       | 3/10 [00:05<00:13,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(74.8372)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  40%|████      | 4/10 [00:07<00:11,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(77.4812)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  50%|█████     | 5/10 [00:09<00:09,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(81.0923)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  60%|██████    | 6/10 [00:11<00:08,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(85.3483)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  70%|███████   | 7/10 [00:13<00:05,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(88.4471)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  80%|████████  | 8/10 [00:15<00:03,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(89.3949)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  90%|█████████ | 9/10 [00:18<00:02,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(87.8062)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 10/10 [00:21<00:00,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(87.7215)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mocked_control_image = torch.zeros(1, 4, 32, 32)\n",
    "    cond = {\n",
    "        \"c_concat\": control, ## Not used\n",
    "        \"c_crossattn\": [object_embedding],\n",
    "    }\n",
    "    un_cond = {\n",
    "        \"c_concat\": control, ## Not used\n",
    "        \"c_crossattn\": [negative_object_embedding],\n",
    "    }\n",
    "\n",
    "    samples, intermediates = sampler.sample(\n",
    "        S=inference_steps,\n",
    "        batch_size=1,\n",
    "        shape=(4, 32, 32),\n",
    "        conditioning=cond,\n",
    "        x_T=initial_latents.clone(),\n",
    "        verbose=False,\n",
    "        unconditional_guidance_scale=scale,\n",
    "        unconditional_conditioning=un_cond,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anydoor_refiners.model import AnyDoor,solver_params\n",
    "from refiners.foundationals.latent_diffusion.solvers import DDIM\n",
    "from tests.mocks import DINOv2EncoderMock,ControlNetMock,AnydoorAutoencoderMock\n",
    "\n",
    "refiners_model = AnyDoor(\n",
    "    lda=AnydoorAutoencoderMock(),\n",
    "    object_encoder=DINOv2EncoderMock(object_embedding,negative_object_embedding),\n",
    "    control_model=ControlNetMock(control),\n",
    "    solver=DDIM(inference_steps,params=solver_params)\n",
    ")\n",
    "# refiners_model.unet.load_state_dict(weights_refiners)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from utils.weight_mapper import get_converted_state_dict\n",
    "\n",
    "with open(\"../tests/weights_mapping/unet.json\", \"r\") as f:\n",
    "    weight_mapping = json.load(f)\n",
    "converted_state_dict = get_converted_state_dict(\n",
    "    source_state_dict=sampler.model.model.diffusion_model.state_dict(),\n",
    "    target_state_dict=refiners_model.unet.state_dict(),\n",
    "    mapping=weight_mapping,\n",
    ")\n",
    "refiners_model.unet.load_state_dict(converted_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:09,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(67.1199)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:02<00:08,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(70.5397)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:03<00:07,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(73.1566)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:04<00:06,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(74.3970)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:05<00:05,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(74.0968)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:07<00:05,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(72.4916)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:09<00:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(69.9599)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:11<00:03,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(66.9328)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:13<00:01,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(61.8021)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(61.4083)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    y = initial_latents.clone()\n",
    "    conditionning = refiners_model.compute_conditionning(object=torch.zeros(1),background=torch.zeros(1))\n",
    "    for s in tqdm(refiners_model.steps):\n",
    "        y = refiners_model.forward(y,step=s,conditionning=conditionning,condition_scale=scale)\n",
    "        print(torch.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(58.3949), tensor(61.4083), tensor(86.1132))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(y-samples[-1]),torch.norm(y),torch.norm(samples[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Anydoor weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_anydoor = get_state_dict_from_safetensors(\"./ckpt/unet.safetensors\")\n",
    "sampler.model.model.diffusion_model.load_state_dict(weights_anydoor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_state_dict = get_converted_state_dict(\n",
    "    source_state_dict=sampler.model.model.diffusion_model.state_dict(),\n",
    "    target_state_dict=refiners_model.unet.state_dict(),\n",
    "    mapping=weight_mapping,\n",
    ")\n",
    "refiners_model.unet.load_state_dict(converted_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mocked_control_image = torch.zeros(1, 4, 32, 32)\n",
    "    cond = {\n",
    "        \"c_concat\": control, ## Not used\n",
    "        \"c_crossattn\": [object_embedding],\n",
    "    }\n",
    "    un_cond = {\n",
    "        \"c_concat\": control, ## Not used\n",
    "        \"c_crossattn\": [negative_object_embedding],\n",
    "    }\n",
    "\n",
    "    samples, intermediates = sampler.sample(\n",
    "        S=inference_steps,\n",
    "        batch_size=1,\n",
    "        shape=(4, 32, 32),\n",
    "        conditioning=cond,\n",
    "        x_T=initial_latents.clone(),\n",
    "        verbose=False,\n",
    "        unconditional_guidance_scale=scale,\n",
    "        unconditional_conditioning=un_cond,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    y = initial_latents.clone()\n",
    "    conditionning = refiners_model.compute_conditionning(object=torch.zeros(1),background=torch.zeros(1))\n",
    "    for s in tqdm(refiners_model.steps):\n",
    "        y = refiners_model.forward(y,step=s,conditionning=conditionning,condition_scale=scale)\n",
    "        print(torch.norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils.weight_mapper import get_converted_state_dict\n",
    "from anydoor_refiners.unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(\"./tests/tensors/x.pt\",weights_only=True)\n",
    "initial_latents = torch.randn(1,4,32,32)\n",
    "timestep = torch.full((1,), 960, dtype=torch.long)\n",
    "object_embedding = torch.load(\"./tests/tensors/object_embedding.pt\",weights_only=True)\n",
    "negative_object_embedding = torch.load(\"./tests/tensors/negative_object_embedding.pt\",weights_only=True)\n",
    "control = torch.load(\"./tests/tensors/control_features.pt\",weights_only=True)\n",
    "inference_steps = 10\n",
    "scale = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n",
      "ControlLDM: Running in eps-prediction mode\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "Loaded model config from [./src/anydoor_original/configs/anydoor.yaml]\n",
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from anydoor_original.cldm import model as cldm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    cond = {\n",
    "        \"c_concat\": control, ## Not used\n",
    "        \"c_crossattn\": [object_embedding],\n",
    "    }\n",
    "    y1 = cldm.apply_model(\n",
    "        x_noisy = x, \n",
    "        t = timestep, \n",
    "        cond = cond,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unet = UNet(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open(\"./tests/weights_mapping/unet.json\", \"r\") as f:\n",
    "    weight_mapping = json.load(f)\n",
    "converted_state_dict = get_converted_state_dict(\n",
    "    source_state_dict=cldm.model.diffusion_model.state_dict(),\n",
    "    target_state_dict=unet.state_dict(),\n",
    "    mapping=weight_mapping,\n",
    ")\n",
    "unet.load_state_dict(converted_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    unet.set_control_residuals(control)\n",
    "    unet.set_timestep(timestep)\n",
    "    unet.set_dinov2_object_embedding(object_embedding)\n",
    "    y2 = unet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(y1-y2),torch.norm(y1),torch.norm(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_weights = get_state_dict_from_safetensors(\"./ckpt/unet.safetensors\")\n",
    "cldm.model.diffusion_model.load_state_dict(unet_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_state_dict = get_converted_state_dict(\n",
    "    source_state_dict=cldm.model.diffusion_model.state_dict(),\n",
    "    target_state_dict=unet.state_dict(),\n",
    "    mapping=weight_mapping,\n",
    ")\n",
    "unet.load_state_dict(converted_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    cond = {\n",
    "        \"c_concat\": control, ## Not used\n",
    "        \"c_crossattn\": [object_embedding],\n",
    "    }\n",
    "    y1_bis = cldm.apply_model(\n",
    "        x_noisy = x, \n",
    "        t = timestep, \n",
    "        cond = cond,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    unet.set_control_residuals(control)\n",
    "    unet.set_timestep(timestep)\n",
    "    unet.set_dinov2_object_embedding(object_embedding)\n",
    "    y2_bis = unet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3982e-05), tensor(65.5668), tensor(65.5668))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(y1_bis-y2_bis),torch.norm(y1_bis),torch.norm(y2_bis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpatialTransformer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils.weight_mapper import get_converted_state_dict\n",
    "from anydoor_refiners.attention import CrossAttentionBlock2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AnyDoor.ldm.modules.attention import SpatialTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configuration parameters with descriptive names\n",
    "input_channels = 320  # Number of input channels for the model\n",
    "num_heads = 5  # Number of attention heads\n",
    "head_dim = 64  # Dimension of each attention head\n",
    "num_layers = 1  # Depth of attention layers\n",
    "context_dim = 1024  # Dimension of the context embedding\n",
    "use_linear_projection = True  # Whether to use linear projection in attention\n",
    "\n",
    "# Initialize the SpatialTransformer model\n",
    "spatial_transformer = SpatialTransformer(\n",
    "    in_channels=input_channels,\n",
    "    n_heads=num_heads,\n",
    "    d_head=head_dim,\n",
    "    depth=num_layers,\n",
    "    context_dim=context_dim,\n",
    "    use_linear=use_linear_projection,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the CrossAttentionBlock2d model\n",
    "cross_attention_block = CrossAttentionBlock2d(\n",
    "    channels=input_channels,\n",
    "    context_embedding_dim=context_dim,\n",
    "    context_key=\"key\",  # Key to set the context in cross_attention_block\n",
    "    num_attention_heads=num_heads,\n",
    "    num_attention_layers=num_layers,\n",
    "    num_groups=32,  # Number of groups for grouped attention\n",
    "    use_bias=False,\n",
    "    use_linear_projection=use_linear_projection,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the source model's state dict to match the target model's structure\n",
    "with open(\"tests/weights_mapping/cross_attention_block_2d.json\", \"r\") as f:\n",
    "    weight_mapping = json.load(f)\n",
    "converted_state_dict = get_converted_state_dict(\n",
    "    source_state_dict=spatial_transformer.state_dict(),\n",
    "    target_state_dict=cross_attention_block.state_dict(),\n",
    "    mapping=weight_mapping,\n",
    ")\n",
    "cross_attention_block.load_state_dict(converted_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(570.7159), tensor(570.7162))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input tensors\n",
    "input_channels = 320  # Must match the model's input channel configuration\n",
    "context_dim = 1024  # Must match the model's context dimension configuration\n",
    "input_tensor = torch.randn(1, input_channels, 32, 32)  # Example input tensor\n",
    "context_tensor = torch.randn(1, 1, context_dim)  # Example context tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Set the context for the CrossAttentionBlock2d model\n",
    "    cross_attention_block.set_context(  # noqa: F821\n",
    "        \"cross_attention_block\", {\"key\": context_tensor}\n",
    "    )\n",
    "    # Forward pass through both models\n",
    "    y_target = cross_attention_block.forward(input_tensor)  # noqa: F821\n",
    "    y_source = spatial_transformer.forward(input_tensor, context=context_tensor)\n",
    "    \n",
    "torch.norm(y_target-y_source),torch.norm(y_target),torch.norm(y_source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spatial_transformer_weights = get_state_dict_from_safetensors(\"./ckpt/spatial_transformer.safetensors\")\n",
    "spatial_transformer.load_state_dict(spatial_transformer_weights)\n",
    "converted_state_dict = get_converted_state_dict(\n",
    "    source_state_dict=spatial_transformer.state_dict(),\n",
    "    target_state_dict=cross_attention_block.state_dict(),  # noqa: F821\n",
    "    mapping=weight_mapping,\n",
    ")\n",
    "cross_attention_block.load_state_dict(converted_state_dict)  # noqa: F821\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0002), tensor(528.3328), tensor(528.3333))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input tensors\n",
    "input_channels = 320  # Must match the model's input channel configuration\n",
    "context_dim = 1024  # Must match the model's context dimension configuration\n",
    "input_tensor = torch.randn(1, input_channels, 32, 32)  # Example input tensor\n",
    "context_tensor = torch.randn(1, 1, context_dim)  # Example context tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Set the context for the CrossAttentionBlock2d model\n",
    "    cross_attention_block.set_context(  # noqa: F821\n",
    "        \"cross_attention_block\", {\"key\": context_tensor}\n",
    "    )\n",
    "    # Forward pass through both models\n",
    "    y_target = cross_attention_block.forward(input_tensor)  # noqa: F821\n",
    "    y_source = spatial_transformer.forward(input_tensor, context=context_tensor)\n",
    "    \n",
    "torch.norm(y_target-y_source),torch.norm(y_target),torch.norm(y_source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_block_keys = [\n",
    "    k for k in spatial_transformer_weights.keys() if \"transformer_blocks.0.\" in k\n",
    "]\n",
    "transformer_block_weights = {\n",
    "    k.replace(\"transformer_blocks.0.\",\"\"): spatial_transformer_weights[k] for k in transformer_block_keys\n",
    "}\n",
    "save_file(transformer_block_weights,\"./ckpt/transformer_block.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_projection_weight_keys = [\n",
    "    'norm.bias', 'norm.weight', 'proj_in.bias', 'proj_in.weight'\n",
    "]\n",
    "input_projection_weights = {\n",
    "    key: spatial_transformer.state_dict()[key]\n",
    "    for key in input_projection_weight_keys\n",
    "}\n",
    "save_file(input_projection_weights, \"./ckpt/input_projection.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection_weight_keys = [\n",
    "    'proj_out.bias', 'proj_out.weight'\n",
    "]\n",
    "output_projection_weights = {\n",
    "    key: spatial_transformer.state_dict()[key]\n",
    "    for key in output_projection_weight_keys\n",
    "}\n",
    "save_file(output_projection_weights, \"./ckpt/output_projection.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Projection\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import refiners.fluxion.layers as fl\n",
    "from torch import nn\n",
    "from utils.weight_mapper import get_converted_state_dict\n",
    "from einops import rearrange\n",
    "from refiners.fluxion.context import Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(num_groups=32, num_channels=320, eps=1e-6, affine=True)\n",
    "        self.proj_in = nn.Linear(320, 320)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = rearrange(x, \"b c h w -> b (h w) c\").contiguous()\n",
    "        x = self.proj_in(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class StatefulFlatten(fl.Chain):\n",
    "    def __init__(\n",
    "        self, context: str, key: str, start_dim: int = 0, end_dim: int = -1\n",
    "    ) -> None:\n",
    "        self.start_dim = start_dim\n",
    "        self.end_dim = end_dim\n",
    "\n",
    "        super().__init__(\n",
    "            fl.SetContext(context=context, key=key, callback=self.push),\n",
    "            fl.Flatten(start_dim=start_dim, end_dim=end_dim),\n",
    "        )\n",
    "\n",
    "    def push(self, sizes: list[torch.Size], x: torch.Tensor) -> None:\n",
    "        sizes.append(\n",
    "            x.shape[\n",
    "                slice(\n",
    "                    self.start_dim,\n",
    "                    (\n",
    "                        self.end_dim + 1\n",
    "                        if self.end_dim >= 0\n",
    "                        else x.ndim + self.end_dim + 1\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "class SmallModelRefiners(fl.Chain):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            fl.GroupNorm(\n",
    "                channels=320,\n",
    "                num_groups=32,\n",
    "                eps=1e-6\n",
    "            ),\n",
    "            StatefulFlatten(context=\"flatten\", key=\"sizes\", start_dim=2),\n",
    "            fl.Transpose(1, 2),\n",
    "            fl.Lambda(lambda x: x.contiguous()),\n",
    "            fl.Linear(\n",
    "                in_features=320,\n",
    "                out_features=320,\n",
    "            ),\n",
    "        )\n",
    "    def init_context(self) -> Contexts:\n",
    "        return {\"flatten\": {\"sizes\": []}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "anydoor = SmallModel()\n",
    "refiners = SmallModelRefiners()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {\n",
    "    \"GroupNorm\": \"norm\",\n",
    "    \"Linear\": \"proj_in\",\n",
    "}\n",
    "refiners_state_dict_converted = get_converted_state_dict(\n",
    "    source_state_dict=anydoor.state_dict(),\n",
    "    target_state_dict=refiners.state_dict(),\n",
    "    mapping=mapping,\n",
    ")\n",
    "refiners.load_state_dict(refiners_state_dict_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(331.8626), tensor(331.8626))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input tensors\n",
    "input_channels = 320  # Must match the model's input channel configuration\n",
    "input_tensor = torch.randn(1, input_channels, 32, 32)  # Example input tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_projected_target = anydoor.forward(input_tensor) \n",
    "    y_projected_source = refiners.forward(input_tensor)\n",
    "    \n",
    "torch.norm(y_projected_target-y_projected_source),torch.norm(y_projected_target),torch.norm(y_projected_source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weights = get_state_dict_from_safetensors(\"./ckpt/input_projection.safetensors\")\n",
    "anydoor.load_state_dict(weights)\n",
    "refiners_state_dict_converted = get_converted_state_dict(\n",
    "    source_state_dict=anydoor.state_dict(),\n",
    "    target_state_dict=refiners.state_dict(),\n",
    "    mapping=mapping,\n",
    ")\n",
    "refiners.load_state_dict(refiners_state_dict_converted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(177.1698), tensor(177.1698))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input tensors\n",
    "input_channels = 320  # Must match the model's input channel configuration\n",
    "input_tensor = torch.randn(1, input_channels, 32, 32)  # Example input tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_projected_target = anydoor.forward(input_tensor) \n",
    "    y_projected_source = refiners.forward(input_tensor)\n",
    "    \n",
    "torch.norm(y_projected_target-y_projected_source),torch.norm(y_projected_target),torch.norm(y_projected_source)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerBlock\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils.weight_mapper import get_converted_state_dict\n",
    "from anydoor_refiners.attention import CrossAttentionBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AnyDoor.ldm.modules.attention import BasicTransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configuration parameters with descriptive names\n",
    "input_channels = 320  # Number of input channels for the model\n",
    "num_heads = 5  # Number of attention heads\n",
    "head_dim = 64  # Dimension of each attention head\n",
    "num_layers = 1  # Depth of attention layers\n",
    "context_dim = 1024  # Dimension of the context embedding\n",
    "use_linear_projection = True  # Whether to use linear projection in attention\n",
    "\n",
    "# Initialize the SpatialTransformer model\n",
    "anydoor = BasicTransformerBlock(\n",
    "    dim=input_channels,\n",
    "    n_heads=num_heads,\n",
    "    d_head=head_dim,\n",
    "    context_dim=context_dim,\n",
    "    disable_self_attn=False,\n",
    "    checkpoint=True)\n",
    "\n",
    "refiners = CrossAttentionBlock(\n",
    "    embedding_dim=input_channels,\n",
    "    context_embedding_dim=context_dim,\n",
    "    context_key=\"key\",\n",
    "    num_heads=num_heads,\n",
    "    use_bias=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"Residual_1.SelfAttention.Linear\": \"attn1.to_out.0\",\n",
    "    \"Residual_2.Attention.Linear\": \"attn2.to_out.0\",\n",
    "    \"Residual_1.LayerNorm\": \"norm1\",\n",
    "    \"Residual_2.LayerNorm\": \"norm2\",\n",
    "    \"Residual_3.LayerNorm\": \"norm3\",\n",
    "    \"Residual_1.SelfAttention.Distribute.Linear_1\": \"attn1.to_q\",\n",
    "    \"Residual_1.SelfAttention.Distribute.Linear_2\": \"attn1.to_k\",\n",
    "    \"Residual_1.SelfAttention.Distribute.Linear_3\": \"attn1.to_v\",\n",
    "    \"Residual_2.Attention.Distribute.Linear_1\": \"attn2.to_q\",\n",
    "    \"Residual_2.Attention.Distribute.Linear_2\": \"attn2.to_k\",\n",
    "    \"Residual_2.Attention.Distribute.Linear_3\": \"attn2.to_v\",\n",
    "    \"Residual_3.Linear_1\": \"ff.net.0.proj\",\n",
    "    \"Residual_3.Linear_2\": \"ff.net.2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "converted_state_dict = get_converted_state_dict(\n",
    "    source_state_dict=anydoor.state_dict(),\n",
    "    target_state_dict=refiners.state_dict(),\n",
    "    mapping=mapping,\n",
    ")\n",
    "refiners.load_state_dict(converted_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(21.7846), tensor(21.7846))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input tensors\n",
    "input_channels = 320  # Must match the model's input channel configuration\n",
    "context_dim = 1024  # Must match the model's context dimension configuration\n",
    "input_tensor = torch.randn(1, 1, input_channels)  # Example input tensor\n",
    "context_tensor = torch.randn(1, 1, context_dim)  # Example context tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Set the context for the CrossAttentionBlock2d model\n",
    "    refiners.set_context(  # noqa: F821\n",
    "        \"cross_attention_block\", {\"key\": context_tensor}\n",
    "    )\n",
    "    # Forward pass through both models\n",
    "    y_target = refiners.forward(input_tensor)  # noqa: F821\n",
    "    y_source = anydoor.forward(input_tensor, context=context_tensor)\n",
    "    \n",
    "torch.norm(y_target-y_source),torch.norm(y_target),torch.norm(y_source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weights = get_state_dict_from_safetensors(\"./ckpt/transformer_block.safetensors\")\n",
    "anydoor.load_state_dict(weights)\n",
    "converted_state_dict = get_converted_state_dict(\n",
    "    source_state_dict=anydoor.state_dict(),\n",
    "    target_state_dict=refiners.state_dict(),  # noqa: F821\n",
    "    mapping=mapping,\n",
    ")\n",
    "refiners.load_state_dict(converted_state_dict)  # noqa: F821\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(18.9913), tensor(18.9913))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input tensors\n",
    "input_channels = 320  # Must match the model's input channel configuration\n",
    "context_dim = 1024  # Must match the model's context dimension configuration\n",
    "input_tensor = torch.randn(1, 1, input_channels)  # Example input tensor\n",
    "context_tensor = torch.randn(1, 1, context_dim)  # Example context tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Set the context for the CrossAttentionBlock2d model\n",
    "    refiners.set_context(  # noqa: F821\n",
    "        \"cross_attention_block\", {\"key\": context_tensor}\n",
    "    )\n",
    "    # Forward pass through both models\n",
    "    y_target = refiners.forward(input_tensor)  # noqa: F821\n",
    "    y_source = anydoor.forward(input_tensor, context=context_tensor)\n",
    "    \n",
    "print(torch.allclose(y_target,y_source,rtol=1e-7,atol=1e-7))\n",
    "torch.norm(y_target-y_source),torch.norm(y_target),torch.norm(y_source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_tensor = torch.randn(1, 1, context_dim)  # Example context tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32, True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_projected_target.dtype, y_projected_source.dtype, torch.allclose(y_projected_target,y_projected_source,rtol=1e-12,atol=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_projected_target.is_contiguous(), y_projected_source.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n"
     ]
    }
   ],
   "source": [
    "for weight in refiners.parameters():\n",
    "    print(weight.is_contiguous(), weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n"
     ]
    }
   ],
   "source": [
    "for weight in anydoor.parameters():\n",
    "    print(weight.is_contiguous(), weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0002), tensor(225.7923), tensor(225.7923))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    # Set the context for the CrossAttentionBlock2d model\n",
    "    refiners.set_context(  # noqa: F821\n",
    "        \"cross_attention_block\", {\"key\": context_tensor}\n",
    "    )\n",
    "    # Forward pass through both models\n",
    "    y_transformed_target = refiners.forward(y_projected_target)  # noqa: F821\n",
    "    y_transformed_source = anydoor.forward(y_projected_source, context=context_tensor)\n",
    "    \n",
    "print(torch.allclose(y_transformed_target,y_transformed_source,rtol=1e-7,atol=1e-7))\n",
    "torch.norm(y_transformed_target-y_transformed_source),torch.norm(y_transformed_target),torch.norm(y_transformed_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Projection\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = nn.Linear(320,320)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_target = output_projection(y_target) \n",
    "    y_source = output_projection(y_source)\n",
    "\n",
    "torch.norm(y_target-y_source),torch.norm(y_target),torch.norm(y_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = get_state_dict_from_safetensors(\"./ckpt/output_projection.safetensors\")\n",
    "output_projection.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = nn.Linear(320,320)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_target = output_projection(y_target) \n",
    "    y_source = output_projection(y_source)\n",
    "\n",
    "torch.norm(y_target-y_source),torch.norm(y_target),torch.norm(y_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "sys.path.append(\"./AnyDoor/\")\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "conf = OmegaConf.load(\"src/anydoor_original/configs/anydoor.yaml\")\n",
    "\n",
    "lda_anydoor = instantiate_from_config(conf.model.params.first_stage_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_s/grxhyczx135dcrhc7pzfhkxr0000gn/T/ipykernel_4580/2496509189.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lda_anydoor.load_state_dict(torch.load(\"ckpt/lda_anydoor.ckpt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_anydoor.load_state_dict(torch.load(\"ckpt/lda_anydoor.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anydoor_refiners.model import AnydoorAutoencoder\n",
    "\n",
    "lda_refiners = AnydoorAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_refiners = lda_refiners.load_from_safetensors(\"ckpt/anydoor_refiners_safetensors/lda.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83653863\n",
      "83653863\n"
     ]
    }
   ],
   "source": [
    "# Print nb of trainable parameters of the two models\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(lda_anydoor))\n",
    "print(count_parameters(lda_refiners))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    img = torch.randn(1,3,256,256)\n",
    "    y1 = lda_refiners.forward(img)\n",
    "    y2 = lda_anydoor.forward(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(207.3743)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(y1 - y2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from refiners.conversion.model_converter import ModelConverter\n",
    "\n",
    "converter = ModelConverter(source_model=lda_anydoor,target_model=lda_refiners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models do not have the same number of basic layers:\n",
      "  <class 'torch.nn.modules.conv.Conv2d'>: Source 72 - Target 64\n",
      "  <class 'torch.nn.modules.linear.Linear'>: Source 0 - Target 8\n",
      "Conversion failed at stage 1\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    img = torch.randn(1,3,256,256)\n",
    "    converter.run(source_args=(img,),target_args=(img,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
