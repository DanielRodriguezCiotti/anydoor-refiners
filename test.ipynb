{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "import sys\n",
    "sys.path.append(\"./AnyDoor/\")\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "torch.set_num_threads(2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_dict_from_safetensors(path:str):\n",
    "    \"\"\" Load a state dict from a safetensors file \"\"\"\n",
    "    tensors = {}\n",
    "    with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n",
    "        for key in f.keys():\n",
    "            tensors[key] = f.get_tensor(key)\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set deterministic behavior\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anydoor_refiners.controlnet import ControlNet\n",
    "\n",
    "controlnet = ControlNet(4,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_image = torch.randn(1, 4, 512, 512).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ChainError",
     "evalue": "\nRuntimeError:\n   File \"/home/daniel/code/anydoor-refiners/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nInput type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n---------------\n(CHAIN) InputBlock()\n    ├── >>> Conv2d(in_channels=4, out_channels=16, kernel_size=(3, 3), padding=(1, 1), device=cpu, dtype=float32) | InputBlock.Conv2d_1 #1\n    ├── SiLU() #1\n    ├── Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), device=cpu, dtype=float32) #2\n    ├── SiLU() #2\n    ├── Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), device=cpu, dtype=float32) #3\n    ├── SiLU() #3\n    ├── Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), padding=(1, 1), device=cpu, dtype=float32) #4\n    ├── SiLU() #4\n    ├── Conv2d(in_channels=32, out_channels=96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), device=cpu, dtype=float32) #5\n    ├── SiLU() #5\n0: Tensor(shape=(1, 4, 512, 512), dtype=float32, device=cuda:0, min=-4.65, max=4.72, mean=0.00, std=1.00, norm=1022.83, grad=False)\n---------------\n(CHAIN) ControlNet(in_channels=4)\n    ├── (PASS) TimestepEncoder()\n    │   ├── UseContext(context=diffusion, key=timestep)\n    │   ├── (CHAIN) RangeEncoder(sinusoidal_embedding_dim=320, embedding_dim=1280)\n    │   │   ├── Lambda(compute_sinusoidal_embedding(x: jaxtyping.Int[Tensor, '*batch 1']) -> jaxtyping.Float[Tensor, '*batch 1 embedding_dim'])\n    │   │   ├── Converter(set_device=False)\n    │   │   ├── Linear(in_features=320, out_features=1280, device=cuda:0, dtype=float32) #1\n    │   │   ├── SiLU()\n    │   │   └── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32) #2\n    │   └── SetContext(context=range_adapter, key=timestep_embedding)\n    ├── >>> (CHAIN) InputBlock() | .InputBlock\n0: Tensor(shape=(1, 4, 512, 512), dtype=float32, device=cuda:0, min=-4.65, max=4.72, mean=0.00, std=1.00, norm=1022.83, grad=False)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mChainError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m controlnet\u001b[38;5;241m.\u001b[39mset_dinov2_object_embedding(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m257\u001b[39m, \u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m      2\u001b[0m controlnet\u001b[38;5;241m.\u001b[39mset_timestep(torch\u001b[38;5;241m.\u001b[39mfull((\u001b[38;5;241m1\u001b[39m,), \u001b[38;5;241m960\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m----> 3\u001b[0m control_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mcontrolnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontrol_image\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/anydoor-refiners/.venv/lib/python3.11/site-packages/refiners/fluxion/layers/chain.py:249\u001b[0m, in \u001b[0;36mChain.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    247\u001b[0m intermediate_args: \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 249\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mintermediate_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    251\u001b[0m         result \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mtuple\u001b[39m[Any], result)\n",
      "File \u001b[0;32m~/code/anydoor-refiners/.venv/lib/python3.11/site-packages/refiners/fluxion/layers/chain.py:243\u001b[0m, in \u001b[0;36mChain._call_layer\u001b[0;34m(self, layer, name, *args)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exception_str:\n\u001b[1;32m    241\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ChainError(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mChainError\u001b[0m: \nRuntimeError:\n   File \"/home/daniel/code/anydoor-refiners/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nInput type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n---------------\n(CHAIN) InputBlock()\n    ├── >>> Conv2d(in_channels=4, out_channels=16, kernel_size=(3, 3), padding=(1, 1), device=cpu, dtype=float32) | InputBlock.Conv2d_1 #1\n    ├── SiLU() #1\n    ├── Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), device=cpu, dtype=float32) #2\n    ├── SiLU() #2\n    ├── Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), device=cpu, dtype=float32) #3\n    ├── SiLU() #3\n    ├── Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), padding=(1, 1), device=cpu, dtype=float32) #4\n    ├── SiLU() #4\n    ├── Conv2d(in_channels=32, out_channels=96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), device=cpu, dtype=float32) #5\n    ├── SiLU() #5\n0: Tensor(shape=(1, 4, 512, 512), dtype=float32, device=cuda:0, min=-4.65, max=4.72, mean=0.00, std=1.00, norm=1022.83, grad=False)\n---------------\n(CHAIN) ControlNet(in_channels=4)\n    ├── (PASS) TimestepEncoder()\n    │   ├── UseContext(context=diffusion, key=timestep)\n    │   ├── (CHAIN) RangeEncoder(sinusoidal_embedding_dim=320, embedding_dim=1280)\n    │   │   ├── Lambda(compute_sinusoidal_embedding(x: jaxtyping.Int[Tensor, '*batch 1']) -> jaxtyping.Float[Tensor, '*batch 1 embedding_dim'])\n    │   │   ├── Converter(set_device=False)\n    │   │   ├── Linear(in_features=320, out_features=1280, device=cuda:0, dtype=float32) #1\n    │   │   ├── SiLU()\n    │   │   └── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32) #2\n    │   └── SetContext(context=range_adapter, key=timestep_embedding)\n    ├── >>> (CHAIN) InputBlock() | .InputBlock\n0: Tensor(shape=(1, 4, 512, 512), dtype=float32, device=cuda:0, min=-4.65, max=4.72, mean=0.00, std=1.00, norm=1022.83, grad=False)"
     ]
    }
   ],
   "source": [
    "controlnet.set_dinov2_object_embedding(torch.randn(1, 257, 1024).to(device))\n",
    "controlnet.set_timestep(torch.full((1,), 960, dtype=torch.long).to(device))\n",
    "control_tensor = controlnet.forward(control_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(\"./tests/tensors/x.pt\",weights_only=True)\n",
    "initial_latents = torch.randn(1,4,32,32)\n",
    "object_embedding = torch.load(\"./tests/tensors/object_embedding.pt\",weights_only=True)\n",
    "negative_object_embedding = torch.load(\"./tests/tensors/negative_object_embedding.pt\",weights_only=True)\n",
    "control = torch.load(\"./tests/tensors/control_features.pt\",weights_only=True)\n",
    "inference_steps = 10\n",
    "scale = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n",
      "ControlLDM: Running in eps-prediction mode\n"
     ]
    }
   ],
   "source": [
    "from anydoor_original.cldm import model as cldm\n",
    "from cldm.ddim_hacked import DDIMSampler\n",
    "sampler = DDIMSampler(cldm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 10 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  10%|█         | 1/10 [00:01<00:17,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(68.5324)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  20%|██        | 2/10 [00:03<00:15,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(71.9554)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  30%|███       | 3/10 [00:05<00:13,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(74.8372)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  40%|████      | 4/10 [00:07<00:11,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(77.4812)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  50%|█████     | 5/10 [00:09<00:09,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(81.0923)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  60%|██████    | 6/10 [00:11<00:08,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(85.3483)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  70%|███████   | 7/10 [00:13<00:05,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(88.4471)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  80%|████████  | 8/10 [00:15<00:03,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(89.3949)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  90%|█████████ | 9/10 [00:18<00:02,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(87.8062)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 10/10 [00:21<00:00,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(87.7215)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mocked_control_image = torch.zeros(1, 4, 32, 32)\n",
    "    cond = {\n",
    "        \"c_concat\": control, ## Not used\n",
    "        \"c_crossattn\": [object_embedding],\n",
    "    }\n",
    "    un_cond = {\n",
    "        \"c_concat\": control, ## Not used\n",
    "        \"c_crossattn\": [negative_object_embedding],\n",
    "    }\n",
    "\n",
    "    samples, intermediates = sampler.sample(\n",
    "        S=inference_steps,\n",
    "        batch_size=1,\n",
    "        shape=(4, 32, 32),\n",
    "        conditioning=cond,\n",
    "        x_T=initial_latents.clone(),\n",
    "        verbose=False,\n",
    "        unconditional_guidance_scale=scale,\n",
    "        unconditional_conditioning=un_cond,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anydoor_refiners.model import AnyDoor,solver_params\n",
    "from refiners.foundationals.latent_diffusion.solvers import DDIM\n",
    "from tests.mocks import DINOv2EncoderMock,ControlNetMock,AnydoorAutoencoderMock\n",
    "\n",
    "refiners_model = AnyDoor(\n",
    "    lda=AnydoorAutoencoderMock(),\n",
    "    object_encoder=DINOv2EncoderMock(object_embedding,negative_object_embedding),\n",
    "    control_model=ControlNetMock(control),\n",
    "    solver=DDIM(inference_steps,params=solver_params)\n",
    ")\n",
    "# refiners_model.unet.load_state_dict(weights_refiners)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from utils.weight_mapper import get_converted_state_dict\n",
    "\n",
    "with open(\"../tests/weights_mapping/unet.json\", \"r\") as f:\n",
    "    weight_mapping = json.load(f)\n",
    "converted_state_dict = get_converted_state_dict(\n",
    "    source_state_dict=sampler.model.model.diffusion_model.state_dict(),\n",
    "    target_state_dict=refiners_model.unet.state_dict(),\n",
    "    mapping=weight_mapping,\n",
    ")\n",
    "refiners_model.unet.load_state_dict(converted_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:09,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(67.1199)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:02<00:08,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(70.5397)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:03<00:07,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(73.1566)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:04<00:06,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(74.3970)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:05<00:05,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(74.0968)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:07<00:05,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(72.4916)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:09<00:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(69.9599)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:11<00:03,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(66.9328)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:13<00:01,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(61.8021)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(61.4083)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    y = initial_latents.clone()\n",
    "    conditionning = refiners_model.compute_conditionning(object=torch.zeros(1),background=torch.zeros(1))\n",
    "    for s in tqdm(refiners_model.steps):\n",
    "        y = refiners_model.forward(y,step=s,conditionning=conditionning,condition_scale=scale)\n",
    "        print(torch.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(58.3949), tensor(61.4083), tensor(86.1132))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(y-samples[-1]),torch.norm(y),torch.norm(samples[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Anydoor weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_anydoor = get_state_dict_from_safetensors(\"./ckpt/unet.safetensors\")\n",
    "sampler.model.model.diffusion_model.load_state_dict(weights_anydoor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_state_dict = get_converted_state_dict(\n",
    "    source_state_dict=sampler.model.model.diffusion_model.state_dict(),\n",
    "    target_state_dict=refiners_model.unet.state_dict(),\n",
    "    mapping=weight_mapping,\n",
    ")\n",
    "refiners_model.unet.load_state_dict(converted_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mocked_control_image = torch.zeros(1, 4, 32, 32)\n",
    "    cond = {\n",
    "        \"c_concat\": control, ## Not used\n",
    "        \"c_crossattn\": [object_embedding],\n",
    "    }\n",
    "    un_cond = {\n",
    "        \"c_concat\": control, ## Not used\n",
    "        \"c_crossattn\": [negative_object_embedding],\n",
    "    }\n",
    "\n",
    "    samples, intermediates = sampler.sample(\n",
    "        S=inference_steps,\n",
    "        batch_size=1,\n",
    "        shape=(4, 32, 32),\n",
    "        conditioning=cond,\n",
    "        x_T=initial_latents.clone(),\n",
    "        verbose=False,\n",
    "        unconditional_guidance_scale=scale,\n",
    "        unconditional_conditioning=un_cond,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    y = initial_latents.clone()\n",
    "    conditionning = refiners_model.compute_conditionning(object=torch.zeros(1),background=torch.zeros(1))\n",
    "    for s in tqdm(refiners_model.steps):\n",
    "        y = refiners_model.forward(y,step=s,conditionning=conditionning,condition_scale=scale)\n",
    "        print(torch.norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils.weight_mapper import get_converted_state_dict\n",
    "from anydoor_refiners.unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 4, 64, 64).to(device) ##torch.load(\"./tests/tensors/x.pt\",weights_only=True).to(device)\n",
    "# initial_latents = torch.randn(1,4,32,32).to(device)\n",
    "timestep = torch.full((1,), 960, dtype=torch.long).to(device)\n",
    "object_embedding = torch.randn(1, 257, 1024).to(device)#torch.load(\"./tests/tensors/object_embedding.pt\",weights_only=True).to(device)\n",
    "negative_object_embedding = torch.randn(1, 257, 1024).to(device) #torch.load(\"./tests/tensors/negative_object_embedding.pt\",weights_only=True).to(device)\n",
    "control = [x.to(device) for x in torch.load(\"./tests/tensors/control_features.pt\",weights_only=True)]\n",
    "inference_steps = 10\n",
    "scale = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlLDM: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "Loaded model config from [./src/anydoor_original/configs/anydoor.yaml]\n",
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from anydoor_original.cldm import model as cldm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cldm.to(device)\n",
    "True # Just to avoid printing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 1024, 320]), v shape: torch.Size([1, 1024, 320])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 2, 320]), v shape: torch.Size([1, 2, 320])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 1024, 320]), v shape: torch.Size([1, 1024, 320])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 2, 320]), v shape: torch.Size([1, 2, 320])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 256, 640]), v shape: torch.Size([1, 256, 640])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 2, 640]), v shape: torch.Size([1, 2, 640])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 256, 640]), v shape: torch.Size([1, 256, 640])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 2, 640]), v shape: torch.Size([1, 2, 640])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 64, 1280]), v shape: torch.Size([1, 64, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 2, 1280]), v shape: torch.Size([1, 2, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 64, 1280]), v shape: torch.Size([1, 64, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 2, 1280]), v shape: torch.Size([1, 2, 1280])\n",
      "q shape: torch.Size([1, 16, 1280]), k shape: torch.Size([1, 16, 1280]), v shape: torch.Size([1, 16, 1280])\n",
      "q shape: torch.Size([1, 16, 1280]), k shape: torch.Size([1, 2, 1280]), v shape: torch.Size([1, 2, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 64, 1280]), v shape: torch.Size([1, 64, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 2, 1280]), v shape: torch.Size([1, 2, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 64, 1280]), v shape: torch.Size([1, 64, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 2, 1280]), v shape: torch.Size([1, 2, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 64, 1280]), v shape: torch.Size([1, 64, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 2, 1280]), v shape: torch.Size([1, 2, 1280])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 256, 640]), v shape: torch.Size([1, 256, 640])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 2, 640]), v shape: torch.Size([1, 2, 640])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 256, 640]), v shape: torch.Size([1, 256, 640])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 2, 640]), v shape: torch.Size([1, 2, 640])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 256, 640]), v shape: torch.Size([1, 256, 640])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 2, 640]), v shape: torch.Size([1, 2, 640])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 1024, 320]), v shape: torch.Size([1, 1024, 320])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 2, 320]), v shape: torch.Size([1, 2, 320])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 1024, 320]), v shape: torch.Size([1, 1024, 320])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 2, 320]), v shape: torch.Size([1, 2, 320])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 1024, 320]), v shape: torch.Size([1, 1024, 320])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 2, 320]), v shape: torch.Size([1, 2, 320])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    cond = {\n",
    "        \"c_concat\": control, ## Not used\n",
    "        \"c_crossattn\": [object_embedding],\n",
    "    }\n",
    "    y1 = cldm.apply_model(\n",
    "        x_noisy = x, \n",
    "        t = timestep, \n",
    "        cond = cond,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unet = UNet(4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open(\"./tests/weights_mapping/unet.json\", \"r\") as f:\n",
    "    weight_mapping = json.load(f)\n",
    "converted_state_dict = get_converted_state_dict(\n",
    "    source_state_dict=cldm.model.diffusion_model.state_dict(),\n",
    "    target_state_dict=unet.state_dict(),\n",
    "    mapping=weight_mapping,\n",
    ")\n",
    "unet.load_state_dict(converted_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ChainError",
     "evalue": "\n\nRuntimeError:\nThe size of tensor a (4) must match the size of tensor b (8) at non-singleton dimension 3\n---------------\n(CHAIN) ResidualControlledConcatenator(n=-2)\n    └── >>> (CAT) | UpBlocks.Chain_1.ResidualControlledConcatenator.Concatenate\n        ├── (RES) Residual()\n        │   └── UseContext(context=control, key=residuals)\n        └── (SUM)\n            ├── UseContext(context=unet, key=residuals) #1\n            └── UseContext(context=control, key=residuals) #2\n0: Tensor(shape=(1, 1280, 8, 8), dtype=float32, device=cuda:0, min=-4.78, max=4.26, mean=0.02, std=0.86, norm=247.46, grad=False)\n---------------\n(CHAIN)\n    ├── >>> (CHAIN) ResidualControlledConcatenator(n=-2) | UpBlocks.Chain_1.ResidualControlledConcatenator\n    │   └── (CAT)\n    │       ├── (RES) Residual() ...\n    │       └── (SUM) ...\n    └── (SUM) ResidualBlock(in_channels=2560, out_channels=1280)\n        ├── (CHAIN)\n        │   ├── GroupNorm(num_groups=32, channels=2560, device=cuda:0, dtype=float32) #1\n        │   ├── SiLU() #1\n        │   ├── (SUM) RangeAdapter2d(channels=1280, embedding_dim=1280) ...\n        │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #2\n0: Tensor(shape=(1, 1280, 8, 8), dtype=float32, device=cuda:0, min=-4.78, max=4.26, mean=0.02, std=0.86, norm=247.46, grad=False)\n---------------\n(CHAIN) UpBlocks()\n    ├── >>> (CHAIN) | UpBlocks.Chain_1 #1\n    │   ├── (CHAIN) ResidualControlledConcatenator(n=-2)\n    │   │   └── (CAT) ...\n    │   └── (SUM) ResidualBlock(in_channels=2560, out_channels=1280)\n    │       ├── (CHAIN) ...\n    │       └── Conv2d(in_channels=2560, out_channels=1280, kernel_size=(1, 1), device=cuda:0, dtype=float32)\n    ├── (CHAIN) #2\n    │   ├── (CHAIN) ResidualControlledConcatenator(n=-3)\n    │   │   └── (CAT) ...\n    │   └── (SUM) ResidualBlock(in_channels=2560, out_channels=1280)\n0: Tensor(shape=(1, 1280, 8, 8), dtype=float32, device=cuda:0, min=-4.78, max=4.26, mean=0.02, std=0.86, norm=247.46, grad=False)\n---------------\n(CHAIN) UNet(in_channels=4)\n    ├── (PASS) TimestepEncoder()\n    │   ├── UseContext(context=diffusion, key=timestep)\n    │   ├── (CHAIN) RangeEncoder(sinusoidal_embedding_dim=320, embedding_dim=1280)\n    │   │   ├── Lambda(compute_sinusoidal_embedding(x: jaxtyping.Int[Tensor, '*batch 1']) -> jaxtyping.Float[Tensor, '*batch 1 embedding_dim'])\n    │   │   ├── Converter(set_device=False)\n    │   │   ├── Linear(in_features=320, out_features=1280, device=cuda:0, dtype=float32) #1\n    │   │   ├── SiLU()\n    │   │   └── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32) #2\n    │   └── SetContext(context=range_adapter, key=timestep_embedding)\n    ├── (CHAIN) DownBlocks(in_channels=4)\n0: Tensor(shape=(1, 1280, 8, 8), dtype=float32, device=cuda:0, min=-4.78, max=4.26, mean=0.02, std=0.86, norm=247.46, grad=False)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mChainError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m unet\u001b[38;5;241m.\u001b[39mset_timestep(timestep)\n\u001b[1;32m      4\u001b[0m unet\u001b[38;5;241m.\u001b[39mset_dinov2_object_embedding(object_embedding)\n\u001b[0;32m----> 5\u001b[0m y2 \u001b[38;5;241m=\u001b[39m \u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/anydoor-refiners/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/anydoor-refiners/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/code/anydoor-refiners/.venv/lib/python3.11/site-packages/refiners/fluxion/layers/chain.py:249\u001b[0m, in \u001b[0;36mChain.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    247\u001b[0m intermediate_args: \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 249\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mintermediate_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    251\u001b[0m         result \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mtuple\u001b[39m[Any], result)\n",
      "File \u001b[0;32m~/code/anydoor-refiners/.venv/lib/python3.11/site-packages/refiners/fluxion/layers/chain.py:243\u001b[0m, in \u001b[0;36mChain._call_layer\u001b[0;34m(self, layer, name, *args)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exception_str:\n\u001b[1;32m    241\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ChainError(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mChainError\u001b[0m: \n\nRuntimeError:\nThe size of tensor a (4) must match the size of tensor b (8) at non-singleton dimension 3\n---------------\n(CHAIN) ResidualControlledConcatenator(n=-2)\n    └── >>> (CAT) | UpBlocks.Chain_1.ResidualControlledConcatenator.Concatenate\n        ├── (RES) Residual()\n        │   └── UseContext(context=control, key=residuals)\n        └── (SUM)\n            ├── UseContext(context=unet, key=residuals) #1\n            └── UseContext(context=control, key=residuals) #2\n0: Tensor(shape=(1, 1280, 8, 8), dtype=float32, device=cuda:0, min=-4.78, max=4.26, mean=0.02, std=0.86, norm=247.46, grad=False)\n---------------\n(CHAIN)\n    ├── >>> (CHAIN) ResidualControlledConcatenator(n=-2) | UpBlocks.Chain_1.ResidualControlledConcatenator\n    │   └── (CAT)\n    │       ├── (RES) Residual() ...\n    │       └── (SUM) ...\n    └── (SUM) ResidualBlock(in_channels=2560, out_channels=1280)\n        ├── (CHAIN)\n        │   ├── GroupNorm(num_groups=32, channels=2560, device=cuda:0, dtype=float32) #1\n        │   ├── SiLU() #1\n        │   ├── (SUM) RangeAdapter2d(channels=1280, embedding_dim=1280) ...\n        │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #2\n0: Tensor(shape=(1, 1280, 8, 8), dtype=float32, device=cuda:0, min=-4.78, max=4.26, mean=0.02, std=0.86, norm=247.46, grad=False)\n---------------\n(CHAIN) UpBlocks()\n    ├── >>> (CHAIN) | UpBlocks.Chain_1 #1\n    │   ├── (CHAIN) ResidualControlledConcatenator(n=-2)\n    │   │   └── (CAT) ...\n    │   └── (SUM) ResidualBlock(in_channels=2560, out_channels=1280)\n    │       ├── (CHAIN) ...\n    │       └── Conv2d(in_channels=2560, out_channels=1280, kernel_size=(1, 1), device=cuda:0, dtype=float32)\n    ├── (CHAIN) #2\n    │   ├── (CHAIN) ResidualControlledConcatenator(n=-3)\n    │   │   └── (CAT) ...\n    │   └── (SUM) ResidualBlock(in_channels=2560, out_channels=1280)\n0: Tensor(shape=(1, 1280, 8, 8), dtype=float32, device=cuda:0, min=-4.78, max=4.26, mean=0.02, std=0.86, norm=247.46, grad=False)\n---------------\n(CHAIN) UNet(in_channels=4)\n    ├── (PASS) TimestepEncoder()\n    │   ├── UseContext(context=diffusion, key=timestep)\n    │   ├── (CHAIN) RangeEncoder(sinusoidal_embedding_dim=320, embedding_dim=1280)\n    │   │   ├── Lambda(compute_sinusoidal_embedding(x: jaxtyping.Int[Tensor, '*batch 1']) -> jaxtyping.Float[Tensor, '*batch 1 embedding_dim'])\n    │   │   ├── Converter(set_device=False)\n    │   │   ├── Linear(in_features=320, out_features=1280, device=cuda:0, dtype=float32) #1\n    │   │   ├── SiLU()\n    │   │   └── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32) #2\n    │   └── SetContext(context=range_adapter, key=timestep_embedding)\n    ├── (CHAIN) DownBlocks(in_channels=4)\n0: Tensor(shape=(1, 1280, 8, 8), dtype=float32, device=cuda:0, min=-4.78, max=4.26, mean=0.02, std=0.86, norm=247.46, grad=False)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    unet.set_control_residuals(control)\n",
    "    unet.set_timestep(timestep)\n",
    "    unet.set_dinov2_object_embedding(object_embedding)\n",
    "    y2 = unet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(y1-y2),torch.norm(y1),torch.norm(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_weights = get_state_dict_from_safetensors(\"./ckpt/unet.safetensors\")\n",
    "cldm.model.diffusion_model.load_state_dict(unet_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_state_dict = get_converted_state_dict(\n",
    "    source_state_dict=cldm.model.diffusion_model.state_dict(),\n",
    "    target_state_dict=unet.state_dict(),\n",
    "    mapping=weight_mapping,\n",
    ")\n",
    "unet.load_state_dict(converted_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 1024, 320]), v shape: torch.Size([1, 1024, 320])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 2, 320]), v shape: torch.Size([1, 2, 320])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 1024, 320]), v shape: torch.Size([1, 1024, 320])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 2, 320]), v shape: torch.Size([1, 2, 320])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 256, 640]), v shape: torch.Size([1, 256, 640])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 2, 640]), v shape: torch.Size([1, 2, 640])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 256, 640]), v shape: torch.Size([1, 256, 640])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 2, 640]), v shape: torch.Size([1, 2, 640])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 64, 1280]), v shape: torch.Size([1, 64, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 2, 1280]), v shape: torch.Size([1, 2, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 64, 1280]), v shape: torch.Size([1, 64, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 2, 1280]), v shape: torch.Size([1, 2, 1280])\n",
      "q shape: torch.Size([1, 16, 1280]), k shape: torch.Size([1, 16, 1280]), v shape: torch.Size([1, 16, 1280])\n",
      "q shape: torch.Size([1, 16, 1280]), k shape: torch.Size([1, 2, 1280]), v shape: torch.Size([1, 2, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 64, 1280]), v shape: torch.Size([1, 64, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 2, 1280]), v shape: torch.Size([1, 2, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 64, 1280]), v shape: torch.Size([1, 64, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 2, 1280]), v shape: torch.Size([1, 2, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 64, 1280]), v shape: torch.Size([1, 64, 1280])\n",
      "q shape: torch.Size([1, 64, 1280]), k shape: torch.Size([1, 2, 1280]), v shape: torch.Size([1, 2, 1280])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 256, 640]), v shape: torch.Size([1, 256, 640])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 2, 640]), v shape: torch.Size([1, 2, 640])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 256, 640]), v shape: torch.Size([1, 256, 640])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 2, 640]), v shape: torch.Size([1, 2, 640])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 256, 640]), v shape: torch.Size([1, 256, 640])\n",
      "q shape: torch.Size([1, 256, 640]), k shape: torch.Size([1, 2, 640]), v shape: torch.Size([1, 2, 640])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 1024, 320]), v shape: torch.Size([1, 1024, 320])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 2, 320]), v shape: torch.Size([1, 2, 320])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 1024, 320]), v shape: torch.Size([1, 1024, 320])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 2, 320]), v shape: torch.Size([1, 2, 320])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 1024, 320]), v shape: torch.Size([1, 1024, 320])\n",
      "q shape: torch.Size([1, 1024, 320]), k shape: torch.Size([1, 2, 320]), v shape: torch.Size([1, 2, 320])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    cond = {\n",
    "        \"c_concat\": control, ## Not used\n",
    "        \"c_crossattn\": [object_embedding],\n",
    "    }\n",
    "    y1_bis = cldm.apply_model(\n",
    "        x_noisy = x, \n",
    "        t = timestep, \n",
    "        cond = cond,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    unet.set_control_residuals(control)\n",
    "    unet.set_timestep(timestep)\n",
    "    unet.set_dinov2_object_embedding(object_embedding)\n",
    "    y2_bis = unet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.3351e-05, device='cuda:0'),\n",
       " tensor(65.5668, device='cuda:0'),\n",
       " tensor(65.5668, device='cuda:0'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(y1_bis-y2_bis),torch.norm(y1_bis),torch.norm(y2_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CHAIN) UNet(in_channels=4)\n",
       "    ├── (PASS) TimestepEncoder()\n",
       "    │   ├── UseContext(context=diffusion, key=timestep)\n",
       "    │   ├── (CHAIN) RangeEncoder(sinusoidal_embedding_dim=320, embedding_dim=1280)\n",
       "    │   │   ├── Lambda(compute_sinusoidal_embedding(x: jaxtyping.Int[Tensor, '*batch 1']) -> jaxtyping.Float[Tensor, '*batch 1 embedding_dim'])\n",
       "    │   │   ├── Converter(set_device=False)\n",
       "    │   │   ├── Linear(in_features=320, out_features=1280, device=cuda:0, dtype=float32) #1\n",
       "    │   │   ├── SiLU()\n",
       "    │   │   └── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32) #2\n",
       "    │   └── SetContext(context=range_adapter, key=timestep_embedding)\n",
       "    ├── (CHAIN) DownBlocks(in_channels=4)\n",
       "    │   ├── (CHAIN) #1\n",
       "    │   │   ├── Conv2d(in_channels=4, out_channels=320, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   └── (PASS) ResidualAccumulator(n=0)\n",
       "    │   │       ├── (RES) Residual()\n",
       "    │   │       │   └── UseContext(context=unet, key=residuals)\n",
       "    │   │       └── SetContext(context=unet, key=residuals)\n",
       "    │   ├── (CHAIN) #2\n",
       "    │   │   ├── (SUM) ResidualBlock(in_channels=320, out_channels=320)\n",
       "    │   │   │   ├── (CHAIN)\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=320, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │   ├── SiLU() #1\n",
       "    │   │   │   │   ├── (SUM) RangeAdapter2d(channels=320, embedding_dim=1280)\n",
       "    │   │   │   │   │   ├── Conv2d(in_channels=320, out_channels=320, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │   └── (CHAIN)\n",
       "    │   │   │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │   │   │   │       ├── SiLU()\n",
       "    │   │   │   │   │       ├── Linear(in_features=1280, out_features=320, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │       └── Reshape(shape=(320, 1, 1))\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=320, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   │   ├── SiLU() #2\n",
       "    │   │   │   │   └── Conv2d(in_channels=320, out_channels=320, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   └── Identity()\n",
       "    │   │   ├── (RES) DinoV2CrossAttention(channels=320)\n",
       "    │   │   │   ├── (CHAIN) #1\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, eps=1e-06, channels=320, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   ├── (CHAIN) StatefulFlatten(start_dim=2)\n",
       "    │   │   │   │   │   ├── SetContext(context=flatten, key=sizes)\n",
       "    │   │   │   │   │   └── Flatten(start_dim=2)\n",
       "    │   │   │   │   ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │   │   │   ├── Lambda(<lambda>(x))\n",
       "    │   │   │   │   └── Linear(in_features=320, out_features=320, device=cuda:0, dtype=float32)\n",
       "    │   │   │   ├── (CHAIN) #2\n",
       "    │   │   │   │   └── (CHAIN) CrossAttentionBlock(embedding_dim=320, context_embedding_dim=1024, context_key=dinov2_object_embedding, num_heads=5, use_bias=False)\n",
       "    │   │   │   │       ├── (RES) Residual() #1\n",
       "    │   │   │   │       │   ├── LayerNorm(normalized_shape=(320,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │       │   └── (CHAIN) SelfAttention(embedding_dim=320, num_heads=5, inner_dim=320, use_bias=False, is_optimized=False) ...\n",
       "    │   │   │   │       ├── (RES) Residual() #2\n",
       "    │   │   │   │       │   ├── LayerNorm(normalized_shape=(320,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │       │   ├── (PAR) ...\n",
       "    │   │   │   │       │   └── (CHAIN) Attention(embedding_dim=320, num_heads=5, key_embedding_dim=1024, value_embedding_dim=1024, inner_dim=320, use_bias=False, is_optimized=False) ...\n",
       "    │   │   │   │       └── (RES) Residual() #3\n",
       "    │   │   │   │           ├── LayerNorm(normalized_shape=(320,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │           ├── Linear(in_features=320, out_features=2560, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │           ├── GLU() ...\n",
       "    │   │   │   │           └── Linear(in_features=1280, out_features=320, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   └── (CHAIN) #3\n",
       "    │   │   │       ├── Linear(in_features=320, out_features=320, device=cuda:0, dtype=float32)\n",
       "    │   │   │       ├── Lambda(<lambda>(x))\n",
       "    │   │   │       ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │   │       ├── (PAR)\n",
       "    │   │   │       │   ├── Identity()\n",
       "    │   │   │       │   └── UseContext(context=flatten, key=sizes)\n",
       "    │   │   │       └── Unflatten(dim=2)\n",
       "    │   │   └── (PASS) ResidualAccumulator(n=1)\n",
       "    │   │       ├── (RES) Residual()\n",
       "    │   │       │   └── UseContext(context=unet, key=residuals)\n",
       "    │   │       └── SetContext(context=unet, key=residuals)\n",
       "    │   ├── (CHAIN) #3\n",
       "    │   │   ├── (SUM) ResidualBlock(in_channels=320, out_channels=320)\n",
       "    │   │   │   ├── (CHAIN)\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=320, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │   ├── SiLU() #1\n",
       "    │   │   │   │   ├── (SUM) RangeAdapter2d(channels=320, embedding_dim=1280)\n",
       "    │   │   │   │   │   ├── Conv2d(in_channels=320, out_channels=320, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │   └── (CHAIN)\n",
       "    │   │   │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │   │   │   │       ├── SiLU()\n",
       "    │   │   │   │   │       ├── Linear(in_features=1280, out_features=320, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │       └── Reshape(shape=(320, 1, 1))\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=320, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   │   ├── SiLU() #2\n",
       "    │   │   │   │   └── Conv2d(in_channels=320, out_channels=320, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   └── Identity()\n",
       "    │   │   ├── (RES) DinoV2CrossAttention(channels=320)\n",
       "    │   │   │   ├── (CHAIN) #1\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, eps=1e-06, channels=320, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   ├── (CHAIN) StatefulFlatten(start_dim=2)\n",
       "    │   │   │   │   │   ├── SetContext(context=flatten, key=sizes)\n",
       "    │   │   │   │   │   └── Flatten(start_dim=2)\n",
       "    │   │   │   │   ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │   │   │   ├── Lambda(<lambda>(x))\n",
       "    │   │   │   │   └── Linear(in_features=320, out_features=320, device=cuda:0, dtype=float32)\n",
       "    │   │   │   ├── (CHAIN) #2\n",
       "    │   │   │   │   └── (CHAIN) CrossAttentionBlock(embedding_dim=320, context_embedding_dim=1024, context_key=dinov2_object_embedding, num_heads=5, use_bias=False)\n",
       "    │   │   │   │       ├── (RES) Residual() #1\n",
       "    │   │   │   │       │   ├── LayerNorm(normalized_shape=(320,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │       │   └── (CHAIN) SelfAttention(embedding_dim=320, num_heads=5, inner_dim=320, use_bias=False, is_optimized=False) ...\n",
       "    │   │   │   │       ├── (RES) Residual() #2\n",
       "    │   │   │   │       │   ├── LayerNorm(normalized_shape=(320,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │       │   ├── (PAR) ...\n",
       "    │   │   │   │       │   └── (CHAIN) Attention(embedding_dim=320, num_heads=5, key_embedding_dim=1024, value_embedding_dim=1024, inner_dim=320, use_bias=False, is_optimized=False) ...\n",
       "    │   │   │   │       └── (RES) Residual() #3\n",
       "    │   │   │   │           ├── LayerNorm(normalized_shape=(320,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │           ├── Linear(in_features=320, out_features=2560, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │           ├── GLU() ...\n",
       "    │   │   │   │           └── Linear(in_features=1280, out_features=320, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   └── (CHAIN) #3\n",
       "    │   │   │       ├── Linear(in_features=320, out_features=320, device=cuda:0, dtype=float32)\n",
       "    │   │   │       ├── Lambda(<lambda>(x))\n",
       "    │   │   │       ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │   │       ├── (PAR)\n",
       "    │   │   │       │   ├── Identity()\n",
       "    │   │   │       │   └── UseContext(context=flatten, key=sizes)\n",
       "    │   │   │       └── Unflatten(dim=2)\n",
       "    │   │   └── (PASS) ResidualAccumulator(n=2)\n",
       "    │   │       ├── (RES) Residual()\n",
       "    │   │       │   └── UseContext(context=unet, key=residuals)\n",
       "    │   │       └── SetContext(context=unet, key=residuals)\n",
       "    │   ├── (CHAIN) #4\n",
       "    │   │   ├── (CHAIN) Downsample(channels=320, scale_factor=2, padding=1)\n",
       "    │   │   │   ├── SetContext(context=sampling, key=shapes)\n",
       "    │   │   │   └── Conv2d(in_channels=320, out_channels=320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   └── (PASS) ResidualAccumulator(n=3)\n",
       "    │   │       ├── (RES) Residual()\n",
       "    │   │       │   └── UseContext(context=unet, key=residuals)\n",
       "    │   │       └── SetContext(context=unet, key=residuals)\n",
       "    │   ├── (CHAIN) #5\n",
       "    │   │   ├── (SUM) ResidualBlock(in_channels=320, out_channels=640)\n",
       "    │   │   │   ├── (CHAIN)\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=320, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │   ├── SiLU() #1\n",
       "    │   │   │   │   ├── (SUM) RangeAdapter2d(channels=640, embedding_dim=1280)\n",
       "    │   │   │   │   │   ├── Conv2d(in_channels=320, out_channels=640, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │   └── (CHAIN)\n",
       "    │   │   │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │   │   │   │       ├── SiLU()\n",
       "    │   │   │   │   │       ├── Linear(in_features=1280, out_features=640, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │       └── Reshape(shape=(640, 1, 1))\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=640, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   │   ├── SiLU() #2\n",
       "    │   │   │   │   └── Conv2d(in_channels=640, out_channels=640, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   └── Conv2d(in_channels=320, out_channels=640, kernel_size=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   ├── (RES) DinoV2CrossAttention(channels=640)\n",
       "    │   │   │   ├── (CHAIN) #1\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, eps=1e-06, channels=640, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   ├── (CHAIN) StatefulFlatten(start_dim=2)\n",
       "    │   │   │   │   │   ├── SetContext(context=flatten, key=sizes)\n",
       "    │   │   │   │   │   └── Flatten(start_dim=2)\n",
       "    │   │   │   │   ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │   │   │   ├── Lambda(<lambda>(x))\n",
       "    │   │   │   │   └── Linear(in_features=640, out_features=640, device=cuda:0, dtype=float32)\n",
       "    │   │   │   ├── (CHAIN) #2\n",
       "    │   │   │   │   └── (CHAIN) CrossAttentionBlock(embedding_dim=640, context_embedding_dim=1024, context_key=dinov2_object_embedding, num_heads=10, use_bias=False)\n",
       "    │   │   │   │       ├── (RES) Residual() #1\n",
       "    │   │   │   │       │   ├── LayerNorm(normalized_shape=(640,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │       │   └── (CHAIN) SelfAttention(embedding_dim=640, num_heads=10, inner_dim=640, use_bias=False, is_optimized=False) ...\n",
       "    │   │   │   │       ├── (RES) Residual() #2\n",
       "    │   │   │   │       │   ├── LayerNorm(normalized_shape=(640,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │       │   ├── (PAR) ...\n",
       "    │   │   │   │       │   └── (CHAIN) Attention(embedding_dim=640, num_heads=10, key_embedding_dim=1024, value_embedding_dim=1024, inner_dim=640, use_bias=False, is_optimized=False) ...\n",
       "    │   │   │   │       └── (RES) Residual() #3\n",
       "    │   │   │   │           ├── LayerNorm(normalized_shape=(640,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │           ├── Linear(in_features=640, out_features=5120, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │           ├── GLU() ...\n",
       "    │   │   │   │           └── Linear(in_features=2560, out_features=640, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   └── (CHAIN) #3\n",
       "    │   │   │       ├── Linear(in_features=640, out_features=640, device=cuda:0, dtype=float32)\n",
       "    │   │   │       ├── Lambda(<lambda>(x))\n",
       "    │   │   │       ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │   │       ├── (PAR)\n",
       "    │   │   │       │   ├── Identity()\n",
       "    │   │   │       │   └── UseContext(context=flatten, key=sizes)\n",
       "    │   │   │       └── Unflatten(dim=2)\n",
       "    │   │   └── (PASS) ResidualAccumulator(n=4)\n",
       "    │   │       ├── (RES) Residual()\n",
       "    │   │       │   └── UseContext(context=unet, key=residuals)\n",
       "    │   │       └── SetContext(context=unet, key=residuals)\n",
       "    │   ├── (CHAIN) #6\n",
       "    │   │   ├── (SUM) ResidualBlock(in_channels=640, out_channels=640)\n",
       "    │   │   │   ├── (CHAIN)\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=640, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │   ├── SiLU() #1\n",
       "    │   │   │   │   ├── (SUM) RangeAdapter2d(channels=640, embedding_dim=1280)\n",
       "    │   │   │   │   │   ├── Conv2d(in_channels=640, out_channels=640, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │   └── (CHAIN)\n",
       "    │   │   │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │   │   │   │       ├── SiLU()\n",
       "    │   │   │   │   │       ├── Linear(in_features=1280, out_features=640, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │       └── Reshape(shape=(640, 1, 1))\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=640, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   │   ├── SiLU() #2\n",
       "    │   │   │   │   └── Conv2d(in_channels=640, out_channels=640, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   └── Identity()\n",
       "    │   │   ├── (RES) DinoV2CrossAttention(channels=640)\n",
       "    │   │   │   ├── (CHAIN) #1\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, eps=1e-06, channels=640, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   ├── (CHAIN) StatefulFlatten(start_dim=2)\n",
       "    │   │   │   │   │   ├── SetContext(context=flatten, key=sizes)\n",
       "    │   │   │   │   │   └── Flatten(start_dim=2)\n",
       "    │   │   │   │   ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │   │   │   ├── Lambda(<lambda>(x))\n",
       "    │   │   │   │   └── Linear(in_features=640, out_features=640, device=cuda:0, dtype=float32)\n",
       "    │   │   │   ├── (CHAIN) #2\n",
       "    │   │   │   │   └── (CHAIN) CrossAttentionBlock(embedding_dim=640, context_embedding_dim=1024, context_key=dinov2_object_embedding, num_heads=10, use_bias=False)\n",
       "    │   │   │   │       ├── (RES) Residual() #1\n",
       "    │   │   │   │       │   ├── LayerNorm(normalized_shape=(640,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │       │   └── (CHAIN) SelfAttention(embedding_dim=640, num_heads=10, inner_dim=640, use_bias=False, is_optimized=False) ...\n",
       "    │   │   │   │       ├── (RES) Residual() #2\n",
       "    │   │   │   │       │   ├── LayerNorm(normalized_shape=(640,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │       │   ├── (PAR) ...\n",
       "    │   │   │   │       │   └── (CHAIN) Attention(embedding_dim=640, num_heads=10, key_embedding_dim=1024, value_embedding_dim=1024, inner_dim=640, use_bias=False, is_optimized=False) ...\n",
       "    │   │   │   │       └── (RES) Residual() #3\n",
       "    │   │   │   │           ├── LayerNorm(normalized_shape=(640,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │           ├── Linear(in_features=640, out_features=5120, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │           ├── GLU() ...\n",
       "    │   │   │   │           └── Linear(in_features=2560, out_features=640, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   └── (CHAIN) #3\n",
       "    │   │   │       ├── Linear(in_features=640, out_features=640, device=cuda:0, dtype=float32)\n",
       "    │   │   │       ├── Lambda(<lambda>(x))\n",
       "    │   │   │       ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │   │       ├── (PAR)\n",
       "    │   │   │       │   ├── Identity()\n",
       "    │   │   │       │   └── UseContext(context=flatten, key=sizes)\n",
       "    │   │   │       └── Unflatten(dim=2)\n",
       "    │   │   └── (PASS) ResidualAccumulator(n=5)\n",
       "    │   │       ├── (RES) Residual()\n",
       "    │   │       │   └── UseContext(context=unet, key=residuals)\n",
       "    │   │       └── SetContext(context=unet, key=residuals)\n",
       "    │   ├── (CHAIN) #7\n",
       "    │   │   ├── (CHAIN) Downsample(channels=640, scale_factor=2, padding=1)\n",
       "    │   │   │   ├── SetContext(context=sampling, key=shapes)\n",
       "    │   │   │   └── Conv2d(in_channels=640, out_channels=640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   └── (PASS) ResidualAccumulator(n=6)\n",
       "    │   │       ├── (RES) Residual()\n",
       "    │   │       │   └── UseContext(context=unet, key=residuals)\n",
       "    │   │       └── SetContext(context=unet, key=residuals)\n",
       "    │   ├── (CHAIN) #8\n",
       "    │   │   ├── (SUM) ResidualBlock(in_channels=640, out_channels=1280)\n",
       "    │   │   │   ├── (CHAIN)\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=640, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │   ├── SiLU() #1\n",
       "    │   │   │   │   ├── (SUM) RangeAdapter2d(channels=1280, embedding_dim=1280)\n",
       "    │   │   │   │   │   ├── Conv2d(in_channels=640, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │   └── (CHAIN)\n",
       "    │   │   │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │   │   │   │       ├── SiLU()\n",
       "    │   │   │   │   │       ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │       └── Reshape(shape=(1280, 1, 1))\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   │   ├── SiLU() #2\n",
       "    │   │   │   │   └── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   └── Conv2d(in_channels=640, out_channels=1280, kernel_size=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   ├── (RES) DinoV2CrossAttention(channels=1280)\n",
       "    │   │   │   ├── (CHAIN) #1\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, eps=1e-06, channels=1280, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   ├── (CHAIN) StatefulFlatten(start_dim=2)\n",
       "    │   │   │   │   │   ├── SetContext(context=flatten, key=sizes)\n",
       "    │   │   │   │   │   └── Flatten(start_dim=2)\n",
       "    │   │   │   │   ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │   │   │   ├── Lambda(<lambda>(x))\n",
       "    │   │   │   │   └── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │   │   ├── (CHAIN) #2\n",
       "    │   │   │   │   └── (CHAIN) CrossAttentionBlock(embedding_dim=1280, context_embedding_dim=1024, context_key=dinov2_object_embedding, num_heads=20, use_bias=False)\n",
       "    │   │   │   │       ├── (RES) Residual() #1\n",
       "    │   │   │   │       │   ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │       │   └── (CHAIN) SelfAttention(embedding_dim=1280, num_heads=20, inner_dim=1280, use_bias=False, is_optimized=False) ...\n",
       "    │   │   │   │       ├── (RES) Residual() #2\n",
       "    │   │   │   │       │   ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │       │   ├── (PAR) ...\n",
       "    │   │   │   │       │   └── (CHAIN) Attention(embedding_dim=1280, num_heads=20, key_embedding_dim=1024, value_embedding_dim=1024, inner_dim=1280, use_bias=False, is_optimized=False) ...\n",
       "    │   │   │   │       └── (RES) Residual() #3\n",
       "    │   │   │   │           ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │           ├── Linear(in_features=1280, out_features=10240, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │           ├── GLU() ...\n",
       "    │   │   │   │           └── Linear(in_features=5120, out_features=1280, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   └── (CHAIN) #3\n",
       "    │   │   │       ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │   │       ├── Lambda(<lambda>(x))\n",
       "    │   │   │       ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │   │       ├── (PAR)\n",
       "    │   │   │       │   ├── Identity()\n",
       "    │   │   │       │   └── UseContext(context=flatten, key=sizes)\n",
       "    │   │   │       └── Unflatten(dim=2)\n",
       "    │   │   └── (PASS) ResidualAccumulator(n=7)\n",
       "    │   │       ├── (RES) Residual()\n",
       "    │   │       │   └── UseContext(context=unet, key=residuals)\n",
       "    │   │       └── SetContext(context=unet, key=residuals)\n",
       "    │   ├── (CHAIN) #9\n",
       "    │   │   ├── (SUM) ResidualBlock(in_channels=1280, out_channels=1280)\n",
       "    │   │   │   ├── (CHAIN)\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │   ├── SiLU() #1\n",
       "    │   │   │   │   ├── (SUM) RangeAdapter2d(channels=1280, embedding_dim=1280)\n",
       "    │   │   │   │   │   ├── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │   └── (CHAIN)\n",
       "    │   │   │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │   │   │   │       ├── SiLU()\n",
       "    │   │   │   │   │       ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │       └── Reshape(shape=(1280, 1, 1))\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   │   ├── SiLU() #2\n",
       "    │   │   │   │   └── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   └── Identity()\n",
       "    │   │   ├── (RES) DinoV2CrossAttention(channels=1280)\n",
       "    │   │   │   ├── (CHAIN) #1\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, eps=1e-06, channels=1280, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   ├── (CHAIN) StatefulFlatten(start_dim=2)\n",
       "    │   │   │   │   │   ├── SetContext(context=flatten, key=sizes)\n",
       "    │   │   │   │   │   └── Flatten(start_dim=2)\n",
       "    │   │   │   │   ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │   │   │   ├── Lambda(<lambda>(x))\n",
       "    │   │   │   │   └── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │   │   ├── (CHAIN) #2\n",
       "    │   │   │   │   └── (CHAIN) CrossAttentionBlock(embedding_dim=1280, context_embedding_dim=1024, context_key=dinov2_object_embedding, num_heads=20, use_bias=False)\n",
       "    │   │   │   │       ├── (RES) Residual() #1\n",
       "    │   │   │   │       │   ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │       │   └── (CHAIN) SelfAttention(embedding_dim=1280, num_heads=20, inner_dim=1280, use_bias=False, is_optimized=False) ...\n",
       "    │   │   │   │       ├── (RES) Residual() #2\n",
       "    │   │   │   │       │   ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │       │   ├── (PAR) ...\n",
       "    │   │   │   │       │   └── (CHAIN) Attention(embedding_dim=1280, num_heads=20, key_embedding_dim=1024, value_embedding_dim=1024, inner_dim=1280, use_bias=False, is_optimized=False) ...\n",
       "    │   │   │   │       └── (RES) Residual() #3\n",
       "    │   │   │   │           ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │           ├── Linear(in_features=1280, out_features=10240, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │           ├── GLU() ...\n",
       "    │   │   │   │           └── Linear(in_features=5120, out_features=1280, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   └── (CHAIN) #3\n",
       "    │   │   │       ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │   │       ├── Lambda(<lambda>(x))\n",
       "    │   │   │       ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │   │       ├── (PAR)\n",
       "    │   │   │       │   ├── Identity()\n",
       "    │   │   │       │   └── UseContext(context=flatten, key=sizes)\n",
       "    │   │   │       └── Unflatten(dim=2)\n",
       "    │   │   └── (PASS) ResidualAccumulator(n=8)\n",
       "    │   │       ├── (RES) Residual()\n",
       "    │   │       │   └── UseContext(context=unet, key=residuals)\n",
       "    │   │       └── SetContext(context=unet, key=residuals)\n",
       "    │   ├── (CHAIN) #10\n",
       "    │   │   ├── (CHAIN) Downsample(channels=1280, scale_factor=2, padding=1)\n",
       "    │   │   │   ├── SetContext(context=sampling, key=shapes)\n",
       "    │   │   │   └── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   └── (PASS) ResidualAccumulator(n=9)\n",
       "    │   │       ├── (RES) Residual()\n",
       "    │   │       │   └── UseContext(context=unet, key=residuals)\n",
       "    │   │       └── SetContext(context=unet, key=residuals)\n",
       "    │   ├── (CHAIN) #11\n",
       "    │   │   ├── (SUM) ResidualBlock(in_channels=1280, out_channels=1280)\n",
       "    │   │   │   ├── (CHAIN)\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │   ├── SiLU() #1\n",
       "    │   │   │   │   ├── (SUM) RangeAdapter2d(channels=1280, embedding_dim=1280)\n",
       "    │   │   │   │   │   ├── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │   └── (CHAIN)\n",
       "    │   │   │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │   │   │   │       ├── SiLU()\n",
       "    │   │   │   │   │       ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │       └── Reshape(shape=(1280, 1, 1))\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   │   ├── SiLU() #2\n",
       "    │   │   │   │   └── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   └── Identity()\n",
       "    │   │   └── (PASS) ResidualAccumulator(n=10)\n",
       "    │   │       ├── (RES) Residual()\n",
       "    │   │       │   └── UseContext(context=unet, key=residuals)\n",
       "    │   │       └── SetContext(context=unet, key=residuals)\n",
       "    │   └── (CHAIN) #12\n",
       "    │       ├── (SUM) ResidualBlock(in_channels=1280, out_channels=1280)\n",
       "    │       │   ├── (CHAIN)\n",
       "    │       │   │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #1\n",
       "    │       │   │   ├── SiLU() #1\n",
       "    │       │   │   ├── (SUM) RangeAdapter2d(channels=1280, embedding_dim=1280)\n",
       "    │       │   │   │   ├── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │       │   │   │   └── (CHAIN)\n",
       "    │       │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │       │   │   │       ├── SiLU()\n",
       "    │       │   │   │       ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │       │   │   │       └── Reshape(shape=(1280, 1, 1))\n",
       "    │       │   │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #2\n",
       "    │       │   │   ├── SiLU() #2\n",
       "    │       │   │   └── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │       │   └── Identity()\n",
       "    │       └── (PASS) ResidualAccumulator(n=11)\n",
       "    │           ├── (RES) Residual()\n",
       "    │           │   └── UseContext(context=unet, key=residuals)\n",
       "    │           └── SetContext(context=unet, key=residuals)\n",
       "    ├── (SUM)\n",
       "    │   ├── UseContext(context=unet, key=residuals)\n",
       "    │   └── (CHAIN) MiddleBlock()\n",
       "    │       ├── (SUM) ResidualBlock(in_channels=1280, out_channels=1280) #1\n",
       "    │       │   ├── (CHAIN)\n",
       "    │       │   │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #1\n",
       "    │       │   │   ├── SiLU() #1\n",
       "    │       │   │   ├── (SUM) RangeAdapter2d(channels=1280, embedding_dim=1280)\n",
       "    │       │   │   │   ├── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │       │   │   │   └── (CHAIN)\n",
       "    │       │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │       │   │   │       ├── SiLU()\n",
       "    │       │   │   │       ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │       │   │   │       └── Reshape(shape=(1280, 1, 1))\n",
       "    │       │   │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #2\n",
       "    │       │   │   ├── SiLU() #2\n",
       "    │       │   │   └── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │       │   └── Identity()\n",
       "    │       ├── (RES) DinoV2CrossAttention(channels=1280)\n",
       "    │       │   ├── (CHAIN) #1\n",
       "    │       │   │   ├── GroupNorm(num_groups=32, eps=1e-06, channels=1280, device=cuda:0, dtype=float32)\n",
       "    │       │   │   ├── (CHAIN) StatefulFlatten(start_dim=2)\n",
       "    │       │   │   │   ├── SetContext(context=flatten, key=sizes)\n",
       "    │       │   │   │   └── Flatten(start_dim=2)\n",
       "    │       │   │   ├── Transpose(dim0=1, dim1=2)\n",
       "    │       │   │   ├── Lambda(<lambda>(x))\n",
       "    │       │   │   └── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │       │   ├── (CHAIN) #2\n",
       "    │       │   │   └── (CHAIN) CrossAttentionBlock(embedding_dim=1280, context_embedding_dim=1024, context_key=dinov2_object_embedding, num_heads=20, use_bias=False)\n",
       "    │       │   │       ├── (RES) Residual() #1\n",
       "    │       │   │       │   ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │       │   │       │   └── (CHAIN) SelfAttention(embedding_dim=1280, num_heads=20, inner_dim=1280, use_bias=False, is_optimized=False) ...\n",
       "    │       │   │       ├── (RES) Residual() #2\n",
       "    │       │   │       │   ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │       │   │       │   ├── (PAR) ...\n",
       "    │       │   │       │   └── (CHAIN) Attention(embedding_dim=1280, num_heads=20, key_embedding_dim=1024, value_embedding_dim=1024, inner_dim=1280, use_bias=False, is_optimized=False) ...\n",
       "    │       │   │       └── (RES) Residual() #3\n",
       "    │       │   │           ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │       │   │           ├── Linear(in_features=1280, out_features=10240, device=cuda:0, dtype=float32) #1\n",
       "    │       │   │           ├── GLU() ...\n",
       "    │       │   │           └── Linear(in_features=5120, out_features=1280, device=cuda:0, dtype=float32) #2\n",
       "    │       │   └── (CHAIN) #3\n",
       "    │       │       ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │       │       ├── Lambda(<lambda>(x))\n",
       "    │       │       ├── Transpose(dim0=1, dim1=2)\n",
       "    │       │       ├── (PAR)\n",
       "    │       │       │   ├── Identity()\n",
       "    │       │       │   └── UseContext(context=flatten, key=sizes)\n",
       "    │       │       └── Unflatten(dim=2)\n",
       "    │       └── (SUM) ResidualBlock(in_channels=1280, out_channels=1280) #2\n",
       "    │           ├── (CHAIN)\n",
       "    │           │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #1\n",
       "    │           │   ├── SiLU() #1\n",
       "    │           │   ├── (SUM) RangeAdapter2d(channels=1280, embedding_dim=1280)\n",
       "    │           │   │   ├── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │           │   │   └── (CHAIN)\n",
       "    │           │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │           │   │       ├── SiLU()\n",
       "    │           │   │       ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │           │   │       └── Reshape(shape=(1280, 1, 1))\n",
       "    │           │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #2\n",
       "    │           │   ├── SiLU() #2\n",
       "    │           │   └── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │           └── Identity()\n",
       "    ├── (CHAIN) UpBlocks()\n",
       "    │   ├── (CHAIN) #1\n",
       "    │   │   ├── (CHAIN) ResidualControlledConcatenator(n=-2)\n",
       "    │   │   │   └── (CAT)\n",
       "    │   │   │       ├── (RES) Residual()\n",
       "    │   │   │       │   └── UseContext(context=control, key=residuals)\n",
       "    │   │   │       └── (SUM)\n",
       "    │   │   │           ├── UseContext(context=unet, key=residuals) #1\n",
       "    │   │   │           └── UseContext(context=control, key=residuals) #2\n",
       "    │   │   └── (SUM) ResidualBlock(in_channels=2560, out_channels=1280)\n",
       "    │   │       ├── (CHAIN)\n",
       "    │   │       │   ├── GroupNorm(num_groups=32, channels=2560, device=cuda:0, dtype=float32) #1\n",
       "    │   │       │   ├── SiLU() #1\n",
       "    │   │       │   ├── (SUM) RangeAdapter2d(channels=1280, embedding_dim=1280)\n",
       "    │   │       │   │   ├── Conv2d(in_channels=2560, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │       │   │   └── (CHAIN)\n",
       "    │   │       │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │       │   │       ├── SiLU()\n",
       "    │   │       │   │       ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │       │   │       └── Reshape(shape=(1280, 1, 1))\n",
       "    │   │       │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #2\n",
       "    │   │       │   ├── SiLU() #2\n",
       "    │   │       │   └── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │       └── Conv2d(in_channels=2560, out_channels=1280, kernel_size=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   ├── (CHAIN) #2\n",
       "    │   │   ├── (CHAIN) ResidualControlledConcatenator(n=-3)\n",
       "    │   │   │   └── (CAT)\n",
       "    │   │   │       ├── Identity()\n",
       "    │   │   │       └── (SUM)\n",
       "    │   │   │           ├── UseContext(context=unet, key=residuals) #1\n",
       "    │   │   │           └── UseContext(context=control, key=residuals) #2\n",
       "    │   │   └── (SUM) ResidualBlock(in_channels=2560, out_channels=1280)\n",
       "    │   │       ├── (CHAIN)\n",
       "    │   │       │   ├── GroupNorm(num_groups=32, channels=2560, device=cuda:0, dtype=float32) #1\n",
       "    │   │       │   ├── SiLU() #1\n",
       "    │   │       │   ├── (SUM) RangeAdapter2d(channels=1280, embedding_dim=1280)\n",
       "    │   │       │   │   ├── Conv2d(in_channels=2560, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │       │   │   └── (CHAIN)\n",
       "    │   │       │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │       │   │       ├── SiLU()\n",
       "    │   │       │   │       ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │       │   │       └── Reshape(shape=(1280, 1, 1))\n",
       "    │   │       │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #2\n",
       "    │   │       │   ├── SiLU() #2\n",
       "    │   │       │   └── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │       └── Conv2d(in_channels=2560, out_channels=1280, kernel_size=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   ├── (CHAIN) #3\n",
       "    │   │   ├── (CHAIN) ResidualControlledConcatenator(n=-4)\n",
       "    │   │   │   └── (CAT)\n",
       "    │   │   │       ├── Identity()\n",
       "    │   │   │       └── (SUM)\n",
       "    │   │   │           ├── UseContext(context=unet, key=residuals) #1\n",
       "    │   │   │           └── UseContext(context=control, key=residuals) #2\n",
       "    │   │   ├── (SUM) ResidualBlock(in_channels=2560, out_channels=1280)\n",
       "    │   │   │   ├── (CHAIN)\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=2560, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │   ├── SiLU() #1\n",
       "    │   │   │   │   ├── (SUM) RangeAdapter2d(channels=1280, embedding_dim=1280)\n",
       "    │   │   │   │   │   ├── Conv2d(in_channels=2560, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │   └── (CHAIN)\n",
       "    │   │   │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │   │   │   │       ├── SiLU()\n",
       "    │   │   │   │   │       ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │       └── Reshape(shape=(1280, 1, 1))\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   │   ├── SiLU() #2\n",
       "    │   │   │   │   └── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   └── Conv2d(in_channels=2560, out_channels=1280, kernel_size=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   └── (CHAIN) Upsample(channels=1280)\n",
       "    │   │       ├── (PAR)\n",
       "    │   │       │   ├── Identity()\n",
       "    │   │       │   └── UseContext(context=sampling, key=shapes)\n",
       "    │   │       ├── Interpolate()\n",
       "    │   │       └── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   ├── (CHAIN) #4\n",
       "    │   │   ├── (CHAIN) ResidualControlledConcatenator(n=-5)\n",
       "    │   │   │   └── (CAT)\n",
       "    │   │   │       ├── Identity()\n",
       "    │   │   │       └── (SUM)\n",
       "    │   │   │           ├── UseContext(context=unet, key=residuals) #1\n",
       "    │   │   │           └── UseContext(context=control, key=residuals) #2\n",
       "    │   │   ├── (SUM) ResidualBlock(in_channels=2560, out_channels=1280)\n",
       "    │   │   │   ├── (CHAIN)\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=2560, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │   ├── SiLU() #1\n",
       "    │   │   │   │   ├── (SUM) RangeAdapter2d(channels=1280, embedding_dim=1280)\n",
       "    │   │   │   │   │   ├── Conv2d(in_channels=2560, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │   └── (CHAIN)\n",
       "    │   │   │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │   │   │   │       ├── SiLU()\n",
       "    │   │   │   │   │       ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │       └── Reshape(shape=(1280, 1, 1))\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   │   ├── SiLU() #2\n",
       "    │   │   │   │   └── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   └── Conv2d(in_channels=2560, out_channels=1280, kernel_size=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   └── (RES) DinoV2CrossAttention(channels=1280)\n",
       "    │   │       ├── (CHAIN) #1\n",
       "    │   │       │   ├── GroupNorm(num_groups=32, eps=1e-06, channels=1280, device=cuda:0, dtype=float32)\n",
       "    │   │       │   ├── (CHAIN) StatefulFlatten(start_dim=2)\n",
       "    │   │       │   │   ├── SetContext(context=flatten, key=sizes)\n",
       "    │   │       │   │   └── Flatten(start_dim=2)\n",
       "    │   │       │   ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │       │   ├── Lambda(<lambda>(x))\n",
       "    │   │       │   └── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │       ├── (CHAIN) #2\n",
       "    │   │       │   └── (CHAIN) CrossAttentionBlock(embedding_dim=1280, context_embedding_dim=1024, context_key=dinov2_object_embedding, num_heads=20, use_bias=False)\n",
       "    │   │       │       ├── (RES) Residual() #1\n",
       "    │   │       │       │   ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │   │       │       │   └── (CHAIN) SelfAttention(embedding_dim=1280, num_heads=20, inner_dim=1280, use_bias=False, is_optimized=False) ...\n",
       "    │   │       │       ├── (RES) Residual() #2\n",
       "    │   │       │       │   ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │   │       │       │   ├── (PAR) ...\n",
       "    │   │       │       │   └── (CHAIN) Attention(embedding_dim=1280, num_heads=20, key_embedding_dim=1024, value_embedding_dim=1024, inner_dim=1280, use_bias=False, is_optimized=False) ...\n",
       "    │   │       │       └── (RES) Residual() #3\n",
       "    │   │       │           ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │   │       │           ├── Linear(in_features=1280, out_features=10240, device=cuda:0, dtype=float32) #1\n",
       "    │   │       │           ├── GLU() ...\n",
       "    │   │       │           └── Linear(in_features=5120, out_features=1280, device=cuda:0, dtype=float32) #2\n",
       "    │   │       └── (CHAIN) #3\n",
       "    │   │           ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │           ├── Lambda(<lambda>(x))\n",
       "    │   │           ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │           ├── (PAR)\n",
       "    │   │           │   ├── Identity()\n",
       "    │   │           │   └── UseContext(context=flatten, key=sizes)\n",
       "    │   │           └── Unflatten(dim=2)\n",
       "    │   ├── (CHAIN) #5\n",
       "    │   │   ├── (CHAIN) ResidualControlledConcatenator(n=-6)\n",
       "    │   │   │   └── (CAT)\n",
       "    │   │   │       ├── Identity()\n",
       "    │   │   │       └── (SUM)\n",
       "    │   │   │           ├── UseContext(context=unet, key=residuals) #1\n",
       "    │   │   │           └── UseContext(context=control, key=residuals) #2\n",
       "    │   │   ├── (SUM) ResidualBlock(in_channels=2560, out_channels=1280)\n",
       "    │   │   │   ├── (CHAIN)\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=2560, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │   ├── SiLU() #1\n",
       "    │   │   │   │   ├── (SUM) RangeAdapter2d(channels=1280, embedding_dim=1280)\n",
       "    │   │   │   │   │   ├── Conv2d(in_channels=2560, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │   └── (CHAIN)\n",
       "    │   │   │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │   │   │   │       ├── SiLU()\n",
       "    │   │   │   │   │       ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │       └── Reshape(shape=(1280, 1, 1))\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   │   ├── SiLU() #2\n",
       "    │   │   │   │   └── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   └── Conv2d(in_channels=2560, out_channels=1280, kernel_size=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   └── (RES) DinoV2CrossAttention(channels=1280)\n",
       "    │   │       ├── (CHAIN) #1\n",
       "    │   │       │   ├── GroupNorm(num_groups=32, eps=1e-06, channels=1280, device=cuda:0, dtype=float32)\n",
       "    │   │       │   ├── (CHAIN) StatefulFlatten(start_dim=2)\n",
       "    │   │       │   │   ├── SetContext(context=flatten, key=sizes)\n",
       "    │   │       │   │   └── Flatten(start_dim=2)\n",
       "    │   │       │   ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │       │   ├── Lambda(<lambda>(x))\n",
       "    │   │       │   └── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │       ├── (CHAIN) #2\n",
       "    │   │       │   └── (CHAIN) CrossAttentionBlock(embedding_dim=1280, context_embedding_dim=1024, context_key=dinov2_object_embedding, num_heads=20, use_bias=False)\n",
       "    │   │       │       ├── (RES) Residual() #1\n",
       "    │   │       │       │   ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │   │       │       │   └── (CHAIN) SelfAttention(embedding_dim=1280, num_heads=20, inner_dim=1280, use_bias=False, is_optimized=False) ...\n",
       "    │   │       │       ├── (RES) Residual() #2\n",
       "    │   │       │       │   ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │   │       │       │   ├── (PAR) ...\n",
       "    │   │       │       │   └── (CHAIN) Attention(embedding_dim=1280, num_heads=20, key_embedding_dim=1024, value_embedding_dim=1024, inner_dim=1280, use_bias=False, is_optimized=False) ...\n",
       "    │   │       │       └── (RES) Residual() #3\n",
       "    │   │       │           ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │   │       │           ├── Linear(in_features=1280, out_features=10240, device=cuda:0, dtype=float32) #1\n",
       "    │   │       │           ├── GLU() ...\n",
       "    │   │       │           └── Linear(in_features=5120, out_features=1280, device=cuda:0, dtype=float32) #2\n",
       "    │   │       └── (CHAIN) #3\n",
       "    │   │           ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │           ├── Lambda(<lambda>(x))\n",
       "    │   │           ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │           ├── (PAR)\n",
       "    │   │           │   ├── Identity()\n",
       "    │   │           │   └── UseContext(context=flatten, key=sizes)\n",
       "    │   │           └── Unflatten(dim=2)\n",
       "    │   ├── (CHAIN) #6\n",
       "    │   │   ├── (CHAIN) ResidualControlledConcatenator(n=-7)\n",
       "    │   │   │   └── (CAT)\n",
       "    │   │   │       ├── Identity()\n",
       "    │   │   │       └── (SUM)\n",
       "    │   │   │           ├── UseContext(context=unet, key=residuals) #1\n",
       "    │   │   │           └── UseContext(context=control, key=residuals) #2\n",
       "    │   │   ├── (SUM) ResidualBlock(in_channels=1920, out_channels=1280)\n",
       "    │   │   │   ├── (CHAIN)\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=1920, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │   ├── SiLU() #1\n",
       "    │   │   │   │   ├── (SUM) RangeAdapter2d(channels=1280, embedding_dim=1280)\n",
       "    │   │   │   │   │   ├── Conv2d(in_channels=1920, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │   └── (CHAIN)\n",
       "    │   │   │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │   │   │   │       ├── SiLU()\n",
       "    │   │   │   │   │       ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │       └── Reshape(shape=(1280, 1, 1))\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   │   ├── SiLU() #2\n",
       "    │   │   │   │   └── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   └── Conv2d(in_channels=1920, out_channels=1280, kernel_size=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   ├── (RES) DinoV2CrossAttention(channels=1280)\n",
       "    │   │   │   ├── (CHAIN) #1\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, eps=1e-06, channels=1280, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   ├── (CHAIN) StatefulFlatten(start_dim=2)\n",
       "    │   │   │   │   │   ├── SetContext(context=flatten, key=sizes)\n",
       "    │   │   │   │   │   └── Flatten(start_dim=2)\n",
       "    │   │   │   │   ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │   │   │   ├── Lambda(<lambda>(x))\n",
       "    │   │   │   │   └── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │   │   ├── (CHAIN) #2\n",
       "    │   │   │   │   └── (CHAIN) CrossAttentionBlock(embedding_dim=1280, context_embedding_dim=1024, context_key=dinov2_object_embedding, num_heads=20, use_bias=False)\n",
       "    │   │   │   │       ├── (RES) Residual() #1\n",
       "    │   │   │   │       │   ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │       │   └── (CHAIN) SelfAttention(embedding_dim=1280, num_heads=20, inner_dim=1280, use_bias=False, is_optimized=False) ...\n",
       "    │   │   │   │       ├── (RES) Residual() #2\n",
       "    │   │   │   │       │   ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │       │   ├── (PAR) ...\n",
       "    │   │   │   │       │   └── (CHAIN) Attention(embedding_dim=1280, num_heads=20, key_embedding_dim=1024, value_embedding_dim=1024, inner_dim=1280, use_bias=False, is_optimized=False) ...\n",
       "    │   │   │   │       └── (RES) Residual() #3\n",
       "    │   │   │   │           ├── LayerNorm(normalized_shape=(1280,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │           ├── Linear(in_features=1280, out_features=10240, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │           ├── GLU() ...\n",
       "    │   │   │   │           └── Linear(in_features=5120, out_features=1280, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   └── (CHAIN) #3\n",
       "    │   │   │       ├── Linear(in_features=1280, out_features=1280, device=cuda:0, dtype=float32)\n",
       "    │   │   │       ├── Lambda(<lambda>(x))\n",
       "    │   │   │       ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │   │       ├── (PAR)\n",
       "    │   │   │       │   ├── Identity()\n",
       "    │   │   │       │   └── UseContext(context=flatten, key=sizes)\n",
       "    │   │   │       └── Unflatten(dim=2)\n",
       "    │   │   └── (CHAIN) Upsample(channels=1280)\n",
       "    │   │       ├── (PAR)\n",
       "    │   │       │   ├── Identity()\n",
       "    │   │       │   └── UseContext(context=sampling, key=shapes)\n",
       "    │   │       ├── Interpolate()\n",
       "    │   │       └── Conv2d(in_channels=1280, out_channels=1280, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   ├── (CHAIN) #7\n",
       "    │   │   ├── (CHAIN) ResidualControlledConcatenator(n=-8)\n",
       "    │   │   │   └── (CAT)\n",
       "    │   │   │       ├── Identity()\n",
       "    │   │   │       └── (SUM)\n",
       "    │   │   │           ├── UseContext(context=unet, key=residuals) #1\n",
       "    │   │   │           └── UseContext(context=control, key=residuals) #2\n",
       "    │   │   ├── (SUM) ResidualBlock(in_channels=1920, out_channels=640)\n",
       "    │   │   │   ├── (CHAIN)\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=1920, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │   ├── SiLU() #1\n",
       "    │   │   │   │   ├── (SUM) RangeAdapter2d(channels=640, embedding_dim=1280)\n",
       "    │   │   │   │   │   ├── Conv2d(in_channels=1920, out_channels=640, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │   └── (CHAIN)\n",
       "    │   │   │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │   │   │   │       ├── SiLU()\n",
       "    │   │   │   │   │       ├── Linear(in_features=1280, out_features=640, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │       └── Reshape(shape=(640, 1, 1))\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=640, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   │   ├── SiLU() #2\n",
       "    │   │   │   │   └── Conv2d(in_channels=640, out_channels=640, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   └── Conv2d(in_channels=1920, out_channels=640, kernel_size=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   └── (RES) DinoV2CrossAttention(channels=640)\n",
       "    │   │       ├── (CHAIN) #1\n",
       "    │   │       │   ├── GroupNorm(num_groups=32, eps=1e-06, channels=640, device=cuda:0, dtype=float32)\n",
       "    │   │       │   ├── (CHAIN) StatefulFlatten(start_dim=2)\n",
       "    │   │       │   │   ├── SetContext(context=flatten, key=sizes)\n",
       "    │   │       │   │   └── Flatten(start_dim=2)\n",
       "    │   │       │   ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │       │   ├── Lambda(<lambda>(x))\n",
       "    │   │       │   └── Linear(in_features=640, out_features=640, device=cuda:0, dtype=float32)\n",
       "    │   │       ├── (CHAIN) #2\n",
       "    │   │       │   └── (CHAIN) CrossAttentionBlock(embedding_dim=640, context_embedding_dim=1024, context_key=dinov2_object_embedding, num_heads=10, use_bias=False)\n",
       "    │   │       │       ├── (RES) Residual() #1\n",
       "    │   │       │       │   ├── LayerNorm(normalized_shape=(640,), device=cuda:0, dtype=float32)\n",
       "    │   │       │       │   └── (CHAIN) SelfAttention(embedding_dim=640, num_heads=10, inner_dim=640, use_bias=False, is_optimized=False) ...\n",
       "    │   │       │       ├── (RES) Residual() #2\n",
       "    │   │       │       │   ├── LayerNorm(normalized_shape=(640,), device=cuda:0, dtype=float32)\n",
       "    │   │       │       │   ├── (PAR) ...\n",
       "    │   │       │       │   └── (CHAIN) Attention(embedding_dim=640, num_heads=10, key_embedding_dim=1024, value_embedding_dim=1024, inner_dim=640, use_bias=False, is_optimized=False) ...\n",
       "    │   │       │       └── (RES) Residual() #3\n",
       "    │   │       │           ├── LayerNorm(normalized_shape=(640,), device=cuda:0, dtype=float32)\n",
       "    │   │       │           ├── Linear(in_features=640, out_features=5120, device=cuda:0, dtype=float32) #1\n",
       "    │   │       │           ├── GLU() ...\n",
       "    │   │       │           └── Linear(in_features=2560, out_features=640, device=cuda:0, dtype=float32) #2\n",
       "    │   │       └── (CHAIN) #3\n",
       "    │   │           ├── Linear(in_features=640, out_features=640, device=cuda:0, dtype=float32)\n",
       "    │   │           ├── Lambda(<lambda>(x))\n",
       "    │   │           ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │           ├── (PAR)\n",
       "    │   │           │   ├── Identity()\n",
       "    │   │           │   └── UseContext(context=flatten, key=sizes)\n",
       "    │   │           └── Unflatten(dim=2)\n",
       "    │   ├── (CHAIN) #8\n",
       "    │   │   ├── (CHAIN) ResidualControlledConcatenator(n=-9)\n",
       "    │   │   │   └── (CAT)\n",
       "    │   │   │       ├── Identity()\n",
       "    │   │   │       └── (SUM)\n",
       "    │   │   │           ├── UseContext(context=unet, key=residuals) #1\n",
       "    │   │   │           └── UseContext(context=control, key=residuals) #2\n",
       "    │   │   ├── (SUM) ResidualBlock(in_channels=1280, out_channels=640)\n",
       "    │   │   │   ├── (CHAIN)\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=1280, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │   ├── SiLU() #1\n",
       "    │   │   │   │   ├── (SUM) RangeAdapter2d(channels=640, embedding_dim=1280)\n",
       "    │   │   │   │   │   ├── Conv2d(in_channels=1280, out_channels=640, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │   └── (CHAIN)\n",
       "    │   │   │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │   │   │   │       ├── SiLU()\n",
       "    │   │   │   │   │       ├── Linear(in_features=1280, out_features=640, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │       └── Reshape(shape=(640, 1, 1))\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=640, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   │   ├── SiLU() #2\n",
       "    │   │   │   │   └── Conv2d(in_channels=640, out_channels=640, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   └── Conv2d(in_channels=1280, out_channels=640, kernel_size=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   └── (RES) DinoV2CrossAttention(channels=640)\n",
       "    │   │       ├── (CHAIN) #1\n",
       "    │   │       │   ├── GroupNorm(num_groups=32, eps=1e-06, channels=640, device=cuda:0, dtype=float32)\n",
       "    │   │       │   ├── (CHAIN) StatefulFlatten(start_dim=2)\n",
       "    │   │       │   │   ├── SetContext(context=flatten, key=sizes)\n",
       "    │   │       │   │   └── Flatten(start_dim=2)\n",
       "    │   │       │   ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │       │   ├── Lambda(<lambda>(x))\n",
       "    │   │       │   └── Linear(in_features=640, out_features=640, device=cuda:0, dtype=float32)\n",
       "    │   │       ├── (CHAIN) #2\n",
       "    │   │       │   └── (CHAIN) CrossAttentionBlock(embedding_dim=640, context_embedding_dim=1024, context_key=dinov2_object_embedding, num_heads=10, use_bias=False)\n",
       "    │   │       │       ├── (RES) Residual() #1\n",
       "    │   │       │       │   ├── LayerNorm(normalized_shape=(640,), device=cuda:0, dtype=float32)\n",
       "    │   │       │       │   └── (CHAIN) SelfAttention(embedding_dim=640, num_heads=10, inner_dim=640, use_bias=False, is_optimized=False) ...\n",
       "    │   │       │       ├── (RES) Residual() #2\n",
       "    │   │       │       │   ├── LayerNorm(normalized_shape=(640,), device=cuda:0, dtype=float32)\n",
       "    │   │       │       │   ├── (PAR) ...\n",
       "    │   │       │       │   └── (CHAIN) Attention(embedding_dim=640, num_heads=10, key_embedding_dim=1024, value_embedding_dim=1024, inner_dim=640, use_bias=False, is_optimized=False) ...\n",
       "    │   │       │       └── (RES) Residual() #3\n",
       "    │   │       │           ├── LayerNorm(normalized_shape=(640,), device=cuda:0, dtype=float32)\n",
       "    │   │       │           ├── Linear(in_features=640, out_features=5120, device=cuda:0, dtype=float32) #1\n",
       "    │   │       │           ├── GLU() ...\n",
       "    │   │       │           └── Linear(in_features=2560, out_features=640, device=cuda:0, dtype=float32) #2\n",
       "    │   │       └── (CHAIN) #3\n",
       "    │   │           ├── Linear(in_features=640, out_features=640, device=cuda:0, dtype=float32)\n",
       "    │   │           ├── Lambda(<lambda>(x))\n",
       "    │   │           ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │           ├── (PAR)\n",
       "    │   │           │   ├── Identity()\n",
       "    │   │           │   └── UseContext(context=flatten, key=sizes)\n",
       "    │   │           └── Unflatten(dim=2)\n",
       "    │   ├── (CHAIN) #9\n",
       "    │   │   ├── (CHAIN) ResidualControlledConcatenator(n=-10)\n",
       "    │   │   │   └── (CAT)\n",
       "    │   │   │       ├── Identity()\n",
       "    │   │   │       └── (SUM)\n",
       "    │   │   │           ├── UseContext(context=unet, key=residuals) #1\n",
       "    │   │   │           └── UseContext(context=control, key=residuals) #2\n",
       "    │   │   ├── (SUM) ResidualBlock(in_channels=960, out_channels=640)\n",
       "    │   │   │   ├── (CHAIN)\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=960, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │   ├── SiLU() #1\n",
       "    │   │   │   │   ├── (SUM) RangeAdapter2d(channels=640, embedding_dim=1280)\n",
       "    │   │   │   │   │   ├── Conv2d(in_channels=960, out_channels=640, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │   └── (CHAIN)\n",
       "    │   │   │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │   │   │   │       ├── SiLU()\n",
       "    │   │   │   │   │       ├── Linear(in_features=1280, out_features=640, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │       └── Reshape(shape=(640, 1, 1))\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=640, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   │   ├── SiLU() #2\n",
       "    │   │   │   │   └── Conv2d(in_channels=640, out_channels=640, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   └── Conv2d(in_channels=960, out_channels=640, kernel_size=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   ├── (RES) DinoV2CrossAttention(channels=640)\n",
       "    │   │   │   ├── (CHAIN) #1\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, eps=1e-06, channels=640, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   ├── (CHAIN) StatefulFlatten(start_dim=2)\n",
       "    │   │   │   │   │   ├── SetContext(context=flatten, key=sizes)\n",
       "    │   │   │   │   │   └── Flatten(start_dim=2)\n",
       "    │   │   │   │   ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │   │   │   ├── Lambda(<lambda>(x))\n",
       "    │   │   │   │   └── Linear(in_features=640, out_features=640, device=cuda:0, dtype=float32)\n",
       "    │   │   │   ├── (CHAIN) #2\n",
       "    │   │   │   │   └── (CHAIN) CrossAttentionBlock(embedding_dim=640, context_embedding_dim=1024, context_key=dinov2_object_embedding, num_heads=10, use_bias=False)\n",
       "    │   │   │   │       ├── (RES) Residual() #1\n",
       "    │   │   │   │       │   ├── LayerNorm(normalized_shape=(640,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │       │   └── (CHAIN) SelfAttention(embedding_dim=640, num_heads=10, inner_dim=640, use_bias=False, is_optimized=False) ...\n",
       "    │   │   │   │       ├── (RES) Residual() #2\n",
       "    │   │   │   │       │   ├── LayerNorm(normalized_shape=(640,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │       │   ├── (PAR) ...\n",
       "    │   │   │   │       │   └── (CHAIN) Attention(embedding_dim=640, num_heads=10, key_embedding_dim=1024, value_embedding_dim=1024, inner_dim=640, use_bias=False, is_optimized=False) ...\n",
       "    │   │   │   │       └── (RES) Residual() #3\n",
       "    │   │   │   │           ├── LayerNorm(normalized_shape=(640,), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │           ├── Linear(in_features=640, out_features=5120, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │           ├── GLU() ...\n",
       "    │   │   │   │           └── Linear(in_features=2560, out_features=640, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   └── (CHAIN) #3\n",
       "    │   │   │       ├── Linear(in_features=640, out_features=640, device=cuda:0, dtype=float32)\n",
       "    │   │   │       ├── Lambda(<lambda>(x))\n",
       "    │   │   │       ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │   │       ├── (PAR)\n",
       "    │   │   │       │   ├── Identity()\n",
       "    │   │   │       │   └── UseContext(context=flatten, key=sizes)\n",
       "    │   │   │       └── Unflatten(dim=2)\n",
       "    │   │   └── (CHAIN) Upsample(channels=640)\n",
       "    │   │       ├── (PAR)\n",
       "    │   │       │   ├── Identity()\n",
       "    │   │       │   └── UseContext(context=sampling, key=shapes)\n",
       "    │   │       ├── Interpolate()\n",
       "    │   │       └── Conv2d(in_channels=640, out_channels=640, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   ├── (CHAIN) #10\n",
       "    │   │   ├── (CHAIN) ResidualControlledConcatenator(n=-11)\n",
       "    │   │   │   └── (CAT)\n",
       "    │   │   │       ├── Identity()\n",
       "    │   │   │       └── (SUM)\n",
       "    │   │   │           ├── UseContext(context=unet, key=residuals) #1\n",
       "    │   │   │           └── UseContext(context=control, key=residuals) #2\n",
       "    │   │   ├── (SUM) ResidualBlock(in_channels=960, out_channels=320)\n",
       "    │   │   │   ├── (CHAIN)\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=960, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │   ├── SiLU() #1\n",
       "    │   │   │   │   ├── (SUM) RangeAdapter2d(channels=320, embedding_dim=1280)\n",
       "    │   │   │   │   │   ├── Conv2d(in_channels=960, out_channels=320, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │   └── (CHAIN)\n",
       "    │   │   │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │   │   │   │       ├── SiLU()\n",
       "    │   │   │   │   │       ├── Linear(in_features=1280, out_features=320, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │       └── Reshape(shape=(320, 1, 1))\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=320, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   │   ├── SiLU() #2\n",
       "    │   │   │   │   └── Conv2d(in_channels=320, out_channels=320, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   └── Conv2d(in_channels=960, out_channels=320, kernel_size=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   └── (RES) DinoV2CrossAttention(channels=320)\n",
       "    │   │       ├── (CHAIN) #1\n",
       "    │   │       │   ├── GroupNorm(num_groups=32, eps=1e-06, channels=320, device=cuda:0, dtype=float32)\n",
       "    │   │       │   ├── (CHAIN) StatefulFlatten(start_dim=2)\n",
       "    │   │       │   │   ├── SetContext(context=flatten, key=sizes)\n",
       "    │   │       │   │   └── Flatten(start_dim=2)\n",
       "    │   │       │   ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │       │   ├── Lambda(<lambda>(x))\n",
       "    │   │       │   └── Linear(in_features=320, out_features=320, device=cuda:0, dtype=float32)\n",
       "    │   │       ├── (CHAIN) #2\n",
       "    │   │       │   └── (CHAIN) CrossAttentionBlock(embedding_dim=320, context_embedding_dim=1024, context_key=dinov2_object_embedding, num_heads=5, use_bias=False)\n",
       "    │   │       │       ├── (RES) Residual() #1\n",
       "    │   │       │       │   ├── LayerNorm(normalized_shape=(320,), device=cuda:0, dtype=float32)\n",
       "    │   │       │       │   └── (CHAIN) SelfAttention(embedding_dim=320, num_heads=5, inner_dim=320, use_bias=False, is_optimized=False) ...\n",
       "    │   │       │       ├── (RES) Residual() #2\n",
       "    │   │       │       │   ├── LayerNorm(normalized_shape=(320,), device=cuda:0, dtype=float32)\n",
       "    │   │       │       │   ├── (PAR) ...\n",
       "    │   │       │       │   └── (CHAIN) Attention(embedding_dim=320, num_heads=5, key_embedding_dim=1024, value_embedding_dim=1024, inner_dim=320, use_bias=False, is_optimized=False) ...\n",
       "    │   │       │       └── (RES) Residual() #3\n",
       "    │   │       │           ├── LayerNorm(normalized_shape=(320,), device=cuda:0, dtype=float32)\n",
       "    │   │       │           ├── Linear(in_features=320, out_features=2560, device=cuda:0, dtype=float32) #1\n",
       "    │   │       │           ├── GLU() ...\n",
       "    │   │       │           └── Linear(in_features=1280, out_features=320, device=cuda:0, dtype=float32) #2\n",
       "    │   │       └── (CHAIN) #3\n",
       "    │   │           ├── Linear(in_features=320, out_features=320, device=cuda:0, dtype=float32)\n",
       "    │   │           ├── Lambda(<lambda>(x))\n",
       "    │   │           ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │           ├── (PAR)\n",
       "    │   │           │   ├── Identity()\n",
       "    │   │           │   └── UseContext(context=flatten, key=sizes)\n",
       "    │   │           └── Unflatten(dim=2)\n",
       "    │   ├── (CHAIN) #11\n",
       "    │   │   ├── (CHAIN) ResidualControlledConcatenator(n=-12)\n",
       "    │   │   │   └── (CAT)\n",
       "    │   │   │       ├── Identity()\n",
       "    │   │   │       └── (SUM)\n",
       "    │   │   │           ├── UseContext(context=unet, key=residuals) #1\n",
       "    │   │   │           └── UseContext(context=control, key=residuals) #2\n",
       "    │   │   ├── (SUM) ResidualBlock(in_channels=640, out_channels=320)\n",
       "    │   │   │   ├── (CHAIN)\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=640, device=cuda:0, dtype=float32) #1\n",
       "    │   │   │   │   ├── SiLU() #1\n",
       "    │   │   │   │   ├── (SUM) RangeAdapter2d(channels=320, embedding_dim=1280)\n",
       "    │   │   │   │   │   ├── Conv2d(in_channels=640, out_channels=320, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │   └── (CHAIN)\n",
       "    │   │   │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │   │   │   │   │       ├── SiLU()\n",
       "    │   │   │   │   │       ├── Linear(in_features=1280, out_features=320, device=cuda:0, dtype=float32)\n",
       "    │   │   │   │   │       └── Reshape(shape=(320, 1, 1))\n",
       "    │   │   │   │   ├── GroupNorm(num_groups=32, channels=320, device=cuda:0, dtype=float32) #2\n",
       "    │   │   │   │   ├── SiLU() #2\n",
       "    │   │   │   │   └── Conv2d(in_channels=320, out_channels=320, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   │   └── Conv2d(in_channels=640, out_channels=320, kernel_size=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │   │   └── (RES) DinoV2CrossAttention(channels=320)\n",
       "    │   │       ├── (CHAIN) #1\n",
       "    │   │       │   ├── GroupNorm(num_groups=32, eps=1e-06, channels=320, device=cuda:0, dtype=float32)\n",
       "    │   │       │   ├── (CHAIN) StatefulFlatten(start_dim=2)\n",
       "    │   │       │   │   ├── SetContext(context=flatten, key=sizes)\n",
       "    │   │       │   │   └── Flatten(start_dim=2)\n",
       "    │   │       │   ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │       │   ├── Lambda(<lambda>(x))\n",
       "    │   │       │   └── Linear(in_features=320, out_features=320, device=cuda:0, dtype=float32)\n",
       "    │   │       ├── (CHAIN) #2\n",
       "    │   │       │   └── (CHAIN) CrossAttentionBlock(embedding_dim=320, context_embedding_dim=1024, context_key=dinov2_object_embedding, num_heads=5, use_bias=False)\n",
       "    │   │       │       ├── (RES) Residual() #1\n",
       "    │   │       │       │   ├── LayerNorm(normalized_shape=(320,), device=cuda:0, dtype=float32)\n",
       "    │   │       │       │   └── (CHAIN) SelfAttention(embedding_dim=320, num_heads=5, inner_dim=320, use_bias=False, is_optimized=False) ...\n",
       "    │   │       │       ├── (RES) Residual() #2\n",
       "    │   │       │       │   ├── LayerNorm(normalized_shape=(320,), device=cuda:0, dtype=float32)\n",
       "    │   │       │       │   ├── (PAR) ...\n",
       "    │   │       │       │   └── (CHAIN) Attention(embedding_dim=320, num_heads=5, key_embedding_dim=1024, value_embedding_dim=1024, inner_dim=320, use_bias=False, is_optimized=False) ...\n",
       "    │   │       │       └── (RES) Residual() #3\n",
       "    │   │       │           ├── LayerNorm(normalized_shape=(320,), device=cuda:0, dtype=float32)\n",
       "    │   │       │           ├── Linear(in_features=320, out_features=2560, device=cuda:0, dtype=float32) #1\n",
       "    │   │       │           ├── GLU() ...\n",
       "    │   │       │           └── Linear(in_features=1280, out_features=320, device=cuda:0, dtype=float32) #2\n",
       "    │   │       └── (CHAIN) #3\n",
       "    │   │           ├── Linear(in_features=320, out_features=320, device=cuda:0, dtype=float32)\n",
       "    │   │           ├── Lambda(<lambda>(x))\n",
       "    │   │           ├── Transpose(dim0=1, dim1=2)\n",
       "    │   │           ├── (PAR)\n",
       "    │   │           │   ├── Identity()\n",
       "    │   │           │   └── UseContext(context=flatten, key=sizes)\n",
       "    │   │           └── Unflatten(dim=2)\n",
       "    │   └── (CHAIN) #12\n",
       "    │       ├── (CHAIN) ResidualControlledConcatenator(n=-13)\n",
       "    │       │   └── (CAT)\n",
       "    │       │       ├── Identity()\n",
       "    │       │       └── (SUM)\n",
       "    │       │           ├── UseContext(context=unet, key=residuals) #1\n",
       "    │       │           └── UseContext(context=control, key=residuals) #2\n",
       "    │       ├── (SUM) ResidualBlock(in_channels=640, out_channels=320)\n",
       "    │       │   ├── (CHAIN)\n",
       "    │       │   │   ├── GroupNorm(num_groups=32, channels=640, device=cuda:0, dtype=float32) #1\n",
       "    │       │   │   ├── SiLU() #1\n",
       "    │       │   │   ├── (SUM) RangeAdapter2d(channels=320, embedding_dim=1280)\n",
       "    │       │   │   │   ├── Conv2d(in_channels=640, out_channels=320, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │       │   │   │   └── (CHAIN)\n",
       "    │       │   │   │       ├── UseContext(context=range_adapter, key=timestep_embedding)\n",
       "    │       │   │   │       ├── SiLU()\n",
       "    │       │   │   │       ├── Linear(in_features=1280, out_features=320, device=cuda:0, dtype=float32)\n",
       "    │       │   │   │       └── Reshape(shape=(320, 1, 1))\n",
       "    │       │   │   ├── GroupNorm(num_groups=32, channels=320, device=cuda:0, dtype=float32) #2\n",
       "    │       │   │   ├── SiLU() #2\n",
       "    │       │   │   └── Conv2d(in_channels=320, out_channels=320, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │       │   └── Conv2d(in_channels=640, out_channels=320, kernel_size=(1, 1), device=cuda:0, dtype=float32)\n",
       "    │       └── (RES) DinoV2CrossAttention(channels=320)\n",
       "    │           ├── (CHAIN) #1\n",
       "    │           │   ├── GroupNorm(num_groups=32, eps=1e-06, channels=320, device=cuda:0, dtype=float32)\n",
       "    │           │   ├── (CHAIN) StatefulFlatten(start_dim=2)\n",
       "    │           │   │   ├── SetContext(context=flatten, key=sizes)\n",
       "    │           │   │   └── Flatten(start_dim=2)\n",
       "    │           │   ├── Transpose(dim0=1, dim1=2)\n",
       "    │           │   ├── Lambda(<lambda>(x))\n",
       "    │           │   └── Linear(in_features=320, out_features=320, device=cuda:0, dtype=float32)\n",
       "    │           ├── (CHAIN) #2\n",
       "    │           │   └── (CHAIN) CrossAttentionBlock(embedding_dim=320, context_embedding_dim=1024, context_key=dinov2_object_embedding, num_heads=5, use_bias=False)\n",
       "    │           │       ├── (RES) Residual() #1\n",
       "    │           │       │   ├── LayerNorm(normalized_shape=(320,), device=cuda:0, dtype=float32)\n",
       "    │           │       │   └── (CHAIN) SelfAttention(embedding_dim=320, num_heads=5, inner_dim=320, use_bias=False, is_optimized=False) ...\n",
       "    │           │       ├── (RES) Residual() #2\n",
       "    │           │       │   ├── LayerNorm(normalized_shape=(320,), device=cuda:0, dtype=float32)\n",
       "    │           │       │   ├── (PAR) ...\n",
       "    │           │       │   └── (CHAIN) Attention(embedding_dim=320, num_heads=5, key_embedding_dim=1024, value_embedding_dim=1024, inner_dim=320, use_bias=False, is_optimized=False) ...\n",
       "    │           │       └── (RES) Residual() #3\n",
       "    │           │           ├── LayerNorm(normalized_shape=(320,), device=cuda:0, dtype=float32)\n",
       "    │           │           ├── Linear(in_features=320, out_features=2560, device=cuda:0, dtype=float32) #1\n",
       "    │           │           ├── GLU() ...\n",
       "    │           │           └── Linear(in_features=1280, out_features=320, device=cuda:0, dtype=float32) #2\n",
       "    │           └── (CHAIN) #3\n",
       "    │               ├── Linear(in_features=320, out_features=320, device=cuda:0, dtype=float32)\n",
       "    │               ├── Lambda(<lambda>(x))\n",
       "    │               ├── Transpose(dim0=1, dim1=2)\n",
       "    │               ├── (PAR)\n",
       "    │               │   ├── Identity()\n",
       "    │               │   └── UseContext(context=flatten, key=sizes)\n",
       "    │               └── Unflatten(dim=2)\n",
       "    └── (CHAIN)\n",
       "        ├── GroupNorm(num_groups=32, channels=320, device=cuda:0, dtype=float32)\n",
       "        ├── SiLU()\n",
       "        └── Conv2d(in_channels=320, out_channels=4, kernel_size=(3, 3), padding=(1, 1), device=cuda:0, dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpatialTransformer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils.weight_mapper import get_converted_state_dict\n",
    "from anydoor_refiners.attention import CrossAttentionBlock2d\n",
    "import refiners.fluxion.layers as fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AnyDoor.ldm.modules.attention import SpatialTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n"
     ]
    }
   ],
   "source": [
    "# Define model configuration parameters with descriptive names\n",
    "input_channels = 320  # Number of input channels for the model\n",
    "num_heads = 5  # Number of attention heads\n",
    "head_dim = 64  # Dimension of each attention head\n",
    "num_layers = 1  # Depth of attention layers\n",
    "context_dim = 1024  # Dimension of the context embedding\n",
    "use_linear_projection = True  # Whether to use linear projection in attention\n",
    "\n",
    "# Initialize the SpatialTransformer model\n",
    "spatial_transformer = SpatialTransformer(\n",
    "    in_channels=input_channels,\n",
    "    n_heads=num_heads,\n",
    "    d_head=head_dim,\n",
    "    depth=num_layers,\n",
    "    context_dim=context_dim,\n",
    "    use_linear=use_linear_projection,\n",
    "    use_checkpoint=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the CrossAttentionBlock2d model\n",
    "cross_attention_block = CrossAttentionBlock2d(\n",
    "    channels=input_channels,\n",
    "    context_embedding_dim=context_dim,\n",
    "    context_key=\"key\",  # Key to set the context in cross_attention_block\n",
    "    num_attention_heads=num_heads,\n",
    "    num_attention_layers=num_layers,\n",
    "    num_groups=32,  # Number of groups for grouped attention\n",
    "    use_bias=False,\n",
    "    use_linear_projection=use_linear_projection,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the source model's state dict to match the target model's structure\n",
    "with open(\"tests/weights_mapping/cross_attention_block_2d.json\", \"r\") as f:\n",
    "    weight_mapping = json.load(f)\n",
    "converted_state_dict = get_converted_state_dict(\n",
    "    source_state_dict=spatial_transformer.state_dict(),\n",
    "    target_state_dict=cross_attention_block.state_dict(),\n",
    "    mapping=weight_mapping,\n",
    ")\n",
    "cross_attention_block.load_state_dict(converted_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor(573.0254, device='cuda:0'),\n",
       " tensor(573.0254, device='cuda:0'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input tensors\n",
    "input_channels = 320  # Must match the model's input channel configuration\n",
    "context_dim = 1024  # Must match the model's context dimension configuration\n",
    "input_tensor = torch.randn(1, input_channels, 32, 32).to(device)  # Example input tensor\n",
    "context_tensor = torch.randn(1, 1, context_dim).to(device)  # Example context tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Set the context for the CrossAttentionBlock2d model\n",
    "    cross_attention_block.set_context(  # noqa: F821\n",
    "        \"cross_attention_block\", {\"key\": context_tensor}\n",
    "    )\n",
    "    # Forward pass through both models\n",
    "    y_source = spatial_transformer.forward(input_tensor, context=context_tensor)\n",
    "    y_target = cross_attention_block.forward(input_tensor)  # noqa: F821\n",
    "    \n",
    "torch.norm(y_target-y_source),torch.norm(y_target),torch.norm(y_source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spatial_transformer_weights = get_state_dict_from_safetensors(\"./ckpt/spatial_transformer.safetensors\")\n",
    "spatial_transformer.load_state_dict(spatial_transformer_weights)\n",
    "converted_state_dict = get_converted_state_dict(\n",
    "    source_state_dict=spatial_transformer.state_dict(),\n",
    "    target_state_dict=cross_attention_block.state_dict(),  # noqa: F821\n",
    "    mapping=weight_mapping,\n",
    ")\n",
    "cross_attention_block.load_state_dict(converted_state_dict)  # noqa: F821\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(207.9414, device='cuda:0'),\n",
       " tensor(540.9109, device='cuda:0'),\n",
       " tensor(529.5787, device='cuda:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input tensors\n",
    "input_channels = 320  # Must match the model's input channel configuration\n",
    "context_dim = 1024  # Must match the model's context dimension configuration\n",
    "input_tensor = torch.randn(1, input_channels, 32, 32).to(device)  # Example input tensor\n",
    "context_tensor = torch.randn(1, 1, context_dim).to(device)  # Example context tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Set the context for the CrossAttentionBlock2d model\n",
    "    cross_attention_block.set_context(  # noqa: F821\n",
    "        \"cross_attention_block\", {\"key\": context_tensor}\n",
    "    )\n",
    "    # Forward pass through both models\n",
    "    y_target = cross_attention_block.forward(input_tensor)  # noqa: F821\n",
    "    y_source = spatial_transformer.forward(input_tensor, context=context_tensor)\n",
    "    \n",
    "torch.norm(y_target-y_source),torch.norm(y_target),torch.norm(y_source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_block_keys = [\n",
    "    k for k in spatial_transformer_weights.keys() if \"transformer_blocks.0.\" in k\n",
    "]\n",
    "transformer_block_weights = {\n",
    "    k.replace(\"transformer_blocks.0.\",\"\"): spatial_transformer_weights[k] for k in transformer_block_keys\n",
    "}\n",
    "save_file(transformer_block_weights,\"./ckpt/transformer_block.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_projection_weight_keys = [\n",
    "    'norm.bias', 'norm.weight', 'proj_in.bias', 'proj_in.weight'\n",
    "]\n",
    "input_projection_weights = {\n",
    "    key: spatial_transformer.state_dict()[key]\n",
    "    for key in input_projection_weight_keys\n",
    "}\n",
    "save_file(input_projection_weights, \"./ckpt/input_projection.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection_weight_keys = [\n",
    "    'proj_out.bias', 'proj_out.weight'\n",
    "]\n",
    "output_projection_weights = {\n",
    "    key: spatial_transformer.state_dict()[key]\n",
    "    for key in output_projection_weight_keys\n",
    "}\n",
    "save_file(output_projection_weights, \"./ckpt/output_projection.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Projection\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import refiners.fluxion.layers as fl\n",
    "from torch import nn\n",
    "from utils.weight_mapper import get_converted_state_dict\n",
    "from einops import rearrange\n",
    "from refiners.fluxion.context import Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(num_groups=32, num_channels=320, eps=1e-6, affine=True)\n",
    "        self.proj_in = nn.Linear(320, 320)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = rearrange(x, \"b c h w -> b (h w) c\").contiguous()\n",
    "        x = self.proj_in(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class SmallModelRefiners(fl.Chain):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            fl.GroupNorm(\n",
    "                channels=320,\n",
    "                num_groups=32,\n",
    "                eps=1e-6\n",
    "            ),\n",
    "            fl.Flatten(start_dim=2, end_dim=-1),\n",
    "            fl.Transpose(1, 2),\n",
    "            fl.Lambda(lambda x: x.contiguous()),\n",
    "            fl.Linear(\n",
    "                in_features=320,\n",
    "                out_features=320,\n",
    "            ),\n",
    "        )\n",
    "    def init_context(self) -> Contexts:\n",
    "        return {\"flatten\": {\"sizes\": []}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "anydoor = SmallModel().to(device)\n",
    "refiners = SmallModelRefiners().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {\n",
    "    \"GroupNorm\": \"norm\",\n",
    "    \"Linear\": \"proj_in\",\n",
    "}\n",
    "refiners_state_dict_converted = get_converted_state_dict(\n",
    "    source_state_dict=anydoor.state_dict(),\n",
    "    target_state_dict=refiners.state_dict(),\n",
    "    mapping=mapping,\n",
    ")\n",
    "refiners.load_state_dict(refiners_state_dict_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor(330.6661, device='cuda:0'),\n",
       " tensor(330.6661, device='cuda:0'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input tensors\n",
    "input_channels = 320  # Must match the model's input channel configuration\n",
    "input_tensor = torch.randn(1, input_channels, 32, 32).to(device)  # Example input tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_projected_target = anydoor.forward(input_tensor) \n",
    "    y_projected_source = refiners.forward(input_tensor)\n",
    "    \n",
    "torch.norm(y_projected_target-y_projected_source),torch.norm(y_projected_target),torch.norm(y_projected_source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weights = get_state_dict_from_safetensors(\"./ckpt/input_projection.safetensors\")\n",
    "anydoor.load_state_dict(weights)\n",
    "refiners_state_dict_converted = get_converted_state_dict(\n",
    "    source_state_dict=anydoor.state_dict(),\n",
    "    target_state_dict=refiners.state_dict(),\n",
    "    mapping=mapping,\n",
    ")\n",
    "refiners.load_state_dict(refiners_state_dict_converted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor(176.8964, device='cuda:0'),\n",
       " tensor(176.8964, device='cuda:0'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input tensors\n",
    "input_channels = 320  # Must match the model's input channel configuration\n",
    "input_tensor = torch.randn(1, input_channels, 32, 32).to(device)  # Example input tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_projected_target = anydoor.forward(input_tensor) \n",
    "    y_projected_source = refiners.forward(input_tensor)\n",
    "    \n",
    "torch.norm(y_projected_target-y_projected_source),torch.norm(y_projected_target),torch.norm(y_projected_source)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerBlock\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils.weight_mapper import get_converted_state_dict\n",
    "from anydoor_refiners.attention import CrossAttentionBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AnyDoor.ldm.modules.attention import BasicTransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configuration parameters with descriptive names\n",
    "input_channels = 320  # Number of input channels for the model\n",
    "num_heads = 5  # Number of attention heads\n",
    "head_dim = 64  # Dimension of each attention head\n",
    "num_layers = 1  # Depth of attention layers\n",
    "context_dim = 1024  # Dimension of the context embedding\n",
    "use_linear_projection = True  # Whether to use linear projection in attention\n",
    "\n",
    "# Initialize the SpatialTransformer model\n",
    "anydoor = BasicTransformerBlock(\n",
    "    dim=input_channels,\n",
    "    n_heads=num_heads,\n",
    "    d_head=head_dim,\n",
    "    context_dim=context_dim,\n",
    "    disable_self_attn=False,\n",
    "    checkpoint=True).to(device)\n",
    "\n",
    "refiners = CrossAttentionBlock(\n",
    "    embedding_dim=input_channels,\n",
    "    context_embedding_dim=context_dim,\n",
    "    context_key=\"key\",\n",
    "    num_heads=num_heads,\n",
    "    use_bias=False).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"Residual_1.SelfAttention.Linear\": \"attn1.to_out.0\",\n",
    "    \"Residual_2.Attention.Linear\": \"attn2.to_out.0\",\n",
    "    \"Residual_1.LayerNorm\": \"norm1\",\n",
    "    \"Residual_2.LayerNorm\": \"norm2\",\n",
    "    \"Residual_3.LayerNorm\": \"norm3\",\n",
    "    \"Residual_1.SelfAttention.Distribute.Linear_1\": \"attn1.to_q\",\n",
    "    \"Residual_1.SelfAttention.Distribute.Linear_2\": \"attn1.to_k\",\n",
    "    \"Residual_1.SelfAttention.Distribute.Linear_3\": \"attn1.to_v\",\n",
    "    \"Residual_2.Attention.Distribute.Linear_1\": \"attn2.to_q\",\n",
    "    \"Residual_2.Attention.Distribute.Linear_2\": \"attn2.to_k\",\n",
    "    \"Residual_2.Attention.Distribute.Linear_3\": \"attn2.to_v\",\n",
    "    \"Residual_3.Linear_1\": \"ff.net.0.proj\",\n",
    "    \"Residual_3.Linear_2\": \"ff.net.2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "converted_state_dict = get_converted_state_dict(\n",
    "    source_state_dict=anydoor.state_dict(),\n",
    "    target_state_dict=refiners.state_dict(),\n",
    "    mapping=mapping,\n",
    ")\n",
    "refiners.load_state_dict(converted_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.9120e-06, device='cuda:0'),\n",
       " tensor(27.1595, device='cuda:0'),\n",
       " tensor(27.1595, device='cuda:0'))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input tensors\n",
    "input_channels = 320  # Must match the model's input channel configuration\n",
    "context_dim = 1024  # Must match the model's context dimension configuration\n",
    "input_tensor = torch.randn(1, 2, input_channels).to(device)  # Example input tensor\n",
    "context_tensor = torch.randn(1, 1, context_dim).to(device)  # Example context tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Set the context for the CrossAttentionBlock2d model\n",
    "    refiners.set_context(  # noqa: F821\n",
    "        \"cross_attention_block\", {\"key\": context_tensor}\n",
    "    )\n",
    "    # Forward pass through both models\n",
    "    y_target = refiners.forward(input_tensor)  # noqa: F821\n",
    "    y_source = anydoor.forward(input_tensor, context=context_tensor)\n",
    "    \n",
    "torch.norm(y_target-y_source),torch.norm(y_target),torch.norm(y_source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weights = get_state_dict_from_safetensors(\"./ckpt/transformer_block.safetensors\")\n",
    "anydoor.load_state_dict(weights)\n",
    "converted_state_dict = get_converted_state_dict(\n",
    "    source_state_dict=anydoor.state_dict(),\n",
    "    target_state_dict=refiners.state_dict(),  # noqa: F821\n",
    "    mapping=mapping,\n",
    ")\n",
    "refiners.load_state_dict(converted_state_dict)  # noqa: F821\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor(1719.6464, device='cuda:0'),\n",
       " tensor(1719.6464, device='cuda:0'))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input tensors\n",
    "input_channels = 320  # Must match the model's input channel configuration\n",
    "context_dim = 1024  # Must match the model's context dimension configuration\n",
    "input_tensor = torch.randn(1, 1, input_channels).to(device) * 100  # Example input tensor\n",
    "context_tensor = torch.randn(1, 1, context_dim).to(device)  # Example context tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Set the context for the CrossAttentionBlock2d model\n",
    "    refiners.set_context(  # noqa: F821\n",
    "        \"cross_attention_block\", {\"key\": context_tensor}\n",
    "    )\n",
    "    # Forward pass through both models\n",
    "    y_target = refiners.forward(input_tensor)  # noqa: F821\n",
    "    y_source = anydoor.forward(input_tensor, context=context_tensor)\n",
    "    \n",
    "print(torch.allclose(y_target,y_source,rtol=1e-7,atol=1e-7))\n",
    "torch.norm(y_target-y_source),torch.norm(y_target),torch.norm(y_source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DerangedModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(num_groups=32, num_channels=320, eps=1e-6, affine=True)\n",
    "        self.proj_in = nn.Linear(320, 320)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.norm(x)\n",
    "        x = rearrange(x, \"b c h w -> b (h w) c\").contiguous()\n",
    "        x = self.proj_in(x)\n",
    "        return x\n",
    "\n",
    "model = DerangedModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input tensors\n",
    "input_channels = 320  # Must match the model's input channel configuration\n",
    "context_dim = 1024  # Must match the model's context dimension configuration\n",
    "input_tensor = torch.randn(1, 2, input_channels).to(device)   # Example input tensor\n",
    "context_tensor = torch.randn(1, 1, context_dim).to(device)  # Example context tensor\n",
    "\n",
    "input_tensor_2 = model(torch.randn( 1, input_channels, 32 , 32).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 320]), torch.Size([1, 1024, 320]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape,input_tensor_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor(3.6385e-06, device='cuda:0') tensor(27.5168, device='cuda:0') tensor(27.5168, device='cuda:0')\n",
      "True\n",
      "tensor(0., device='cuda:0') tensor(387.2995, device='cuda:0') tensor(387.2995, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    # Set the context for the CrossAttentionBlock2d model\n",
    "    refiners.set_context(  # noqa: F821\n",
    "        \"cross_attention_block\", {\"key\": context_tensor.clone()}\n",
    "    )\n",
    "    # Forward pass through both models\n",
    "    y1 = refiners.forward(input_tensor)  # noqa: F821\n",
    "    y2 = anydoor.forward(input_tensor, context=context_tensor.clone())\n",
    "    \n",
    "    print(torch.allclose(y1,y2,rtol=1e-7,atol=1e-7)) # type: ignore\n",
    "    print(torch.norm(y1-y2),torch.norm(y1),torch.norm(y2))\n",
    "\n",
    "    # Set the context for the CrossAttentionBlock2d model\n",
    "    refiners.set_context(  # noqa: F821\n",
    "        \"cross_attention_block\", {\"key\": context_tensor.clone()}\n",
    "    )\n",
    "    # Forward pass through both models\n",
    "    y1_bis = refiners.forward(y_projected_target)  # noqa: F821\n",
    "    y2_bis = anydoor.forward(y_projected_target, context=context_tensor.clone())\n",
    "    \n",
    "    print(torch.allclose(y1_bis,y2_bis,rtol=1e-7,atol=1e-7))\n",
    "    print(torch.norm(y1_bis-y2_bis),torch.norm(y1_bis),torch.norm(y2_bis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from refiners.conversion.model_converter import ModelConverter\n",
    "\n",
    "model_converter = ModelConverter(source_model=anydoor, target_model=refiners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 0 -> 1 - Models have the same number of basic layers. Finding matching shapes and layers...\n",
      "Stage 1 -> 2 - Shape of both models agree. Applying state_dict to target model. Comparing models...\n",
      "Models diverged between attn1.to_v and attn1.to_out.0, and between Residual_1.SelfAttention.Distribute.Linear_3 and Residual_1.SelfAttention.Linear, difference in norm: 9.048733045347035e-05\n",
      "Models do not agree. Try to increase the threshold or modify the models.\n",
      "Conversion failed at stage 3\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    refiners.set_context( \n",
    "        \"cross_attention_block\", {\"key\": context_tensor.clone()}\n",
    "    )\n",
    "    model_converter.run(source_args=(y_projected_source, context_tensor), target_args=(y_projected_source,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32, True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_projected_target.dtype, y_projected_source.dtype, torch.allclose(y_projected_target,y_projected_source,rtol=1e-12,atol=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_projected_target.is_contiguous(), y_projected_source.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n"
     ]
    }
   ],
   "source": [
    "for weight in refiners.parameters():\n",
    "    print(weight.is_contiguous(), weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n",
      "True torch.float32\n"
     ]
    }
   ],
   "source": [
    "for weight in anydoor.parameters():\n",
    "    print(weight.is_contiguous(), weight.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import scaled_dot_product_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def scaled_dot_product_attention_non_optimized(\n",
    "    query: Float[Tensor, \"batch source_sequence_length dim\"],\n",
    "    key: Float[Tensor, \"batch target_sequence_length dim\"],\n",
    "    value: Float[Tensor, \"batch target_sequence_length dim\"],\n",
    "    is_causal: bool = False,\n",
    ") -> Float[Tensor, \"batch source_sequence_length dim\"]:\n",
    "    \"\"\"Non-optimized Scaled Dot Product Attention.\n",
    "\n",
    "    See [[arXiv:1706.03762] Attention Is All You Need (Equation 1)](https://arxiv.org/abs/1706.03762) for more details.\n",
    "    \"\"\"\n",
    "    if is_causal:\n",
    "        # TODO: implement causal attention\n",
    "        raise NotImplementedError(\n",
    "            \"Causal attention for `scaled_dot_product_attention_non_optimized` is not yet implemented\"\n",
    "        )\n",
    "\n",
    "    dim = query.shape[-1]\n",
    "    attention = query @ key.permute(0, 1, 3, 2)\n",
    "    attention = attention / math.sqrt(dim)\n",
    "    attention = torch.softmax(input=attention, dim=-1)\n",
    "    return attention @ value\n",
    "\n",
    "class ScaledDotProductAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int = 1,\n",
    "        is_causal: bool = False,\n",
    "        is_optimized: bool = True,\n",
    "        slice_size: int | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.is_causal = is_causal\n",
    "        self.is_optimized = is_optimized\n",
    "        self.slice_size = slice_size\n",
    "        self.dot_product = scaled_dot_product_attention_non_optimized\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: Float[Tensor, \"batch num_queries embedding_dim\"],\n",
    "        key: Float[Tensor, \"batch num_keys embedding_dim\"],\n",
    "        value: Float[Tensor, \"batch num_values embedding_dim\"],\n",
    "    ) -> Float[Tensor, \"batch num_queries embedding_dim\"]:\n",
    "\n",
    "        return self._process_attention(\n",
    "            query=query,\n",
    "            key=key,\n",
    "            value=value,\n",
    "        )\n",
    "\n",
    "\n",
    "    def _process_attention(\n",
    "        self,\n",
    "        query: Float[Tensor, \"batch num_queries embedding_dim\"],\n",
    "        key: Float[Tensor, \"batch num_keys embedding_dim\"],\n",
    "        value: Float[Tensor, \"batch num_values embedding_dim\"],\n",
    "    ) -> Float[Tensor, \"batch num_queries embedding_dim\"]:\n",
    "        return self._merge_multi_head(\n",
    "            x=self.dot_product(\n",
    "                query=self._split_to_multi_head(query),\n",
    "                key=self._split_to_multi_head(key),\n",
    "                value=self._split_to_multi_head(value),\n",
    "                is_causal=self.is_causal,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _split_to_multi_head(\n",
    "        self,\n",
    "        x: Float[Tensor, \"batch_size sequence_length embedding_dim\"],\n",
    "    ) -> Float[Tensor, \"batch_size num_heads sequence_length (embedding_dim//num_heads)\"]:\n",
    "        \"\"\"Split the input tensor into multiple heads along the embedding dimension.\n",
    "\n",
    "        See also `merge_multi_head`, which is the inverse operation.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            x.ndim == 3\n",
    "        ), f\"Expected input tensor with shape (batch_size sequence_length embedding_dim), got {x.shape}\"\n",
    "        assert (\n",
    "            x.shape[-1] % self.num_heads == 0\n",
    "        ), f\"Expected embedding_dim (x.shape[-1]={x.shape[-1]}) to be divisible by num_heads ({self.num_heads})\"\n",
    "\n",
    "        return x.reshape(x.shape[0], x.shape[1], self.num_heads, x.shape[-1] // self.num_heads).transpose(1, 2)\n",
    "\n",
    "    def _merge_multi_head(\n",
    "        self,\n",
    "        x: Float[Tensor, \"batch_size num_heads sequence_length heads_dim\"],\n",
    "    ) -> Float[Tensor, \"batch_size sequence_length heads_dim * num_heads\"]:\n",
    "        \"\"\"Merge the input tensor from multiple heads along the embedding dimension.\n",
    "\n",
    "        See also `split_to_multi_head`, which is the inverse operation.\n",
    "        \"\"\"\n",
    "        return x.transpose(1, 2).reshape(x.shape[0], x.shape[2], self.num_heads * x.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AnyDoorAttentionProduct(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, heads: int):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = 64 ** -0.5\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        h = self.heads\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
    "\n",
    "        with torch.autocast(enabled=False, device_type = 'cuda'):\n",
    "            q, k = q.float(), k.float()\n",
    "            sim = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        \n",
    "        del q, k\n",
    "\n",
    "        # attention, what we cannot get enough of\n",
    "        sim = sim.softmax(dim=-1)\n",
    "\n",
    "        out = torch.einsum('b i j, b j d -> b i d', sim, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "refiners_attention = ScaledDotProductAttention(num_heads=5).to(device)\n",
    "anydoor_attention = AnyDoorAttentionProduct(heads=5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor(23.2752, device='cuda:0'),\n",
       " tensor(23.2752, device='cuda:0'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input tensors\n",
    "dim = 320  \n",
    "q = torch.randn(1, 10, 320).to(device)  # Example input tensor\n",
    "k = torch.randn(1, 10, 320).to(device)  # Example input tensor\n",
    "v = torch.randn(1, 10, 320).to(device)  # Example input tensor\n",
    "\n",
    "anydoor_result = anydoor_attention(q, k, v)\n",
    "refiners_result = refiners_attention(q, k, v)\n",
    "\n",
    "torch.norm(anydoor_result-refiners_result),torch.norm(anydoor_result),torch.norm(refiners_result)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Projection\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = nn.Linear(320,320)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_target = output_projection(y_target) \n",
    "    y_source = output_projection(y_source)\n",
    "\n",
    "torch.norm(y_target-y_source),torch.norm(y_target),torch.norm(y_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = get_state_dict_from_safetensors(\"./ckpt/output_projection.safetensors\")\n",
    "output_projection.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = nn.Linear(320,320)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_target = output_projection(y_target) \n",
    "    y_source = output_projection(y_source)\n",
    "\n",
    "torch.norm(y_target-y_source),torch.norm(y_target),torch.norm(y_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "sys.path.append(\"./AnyDoor/\")\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "conf = OmegaConf.load(\"src/anydoor_original/configs/anydoor.yaml\")\n",
    "\n",
    "lda_anydoor = instantiate_from_config(conf.model.params.first_stage_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_s/grxhyczx135dcrhc7pzfhkxr0000gn/T/ipykernel_4580/2496509189.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lda_anydoor.load_state_dict(torch.load(\"ckpt/lda_anydoor.ckpt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_anydoor.load_state_dict(torch.load(\"ckpt/lda_anydoor.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anydoor_refiners.model import AnydoorAutoencoder\n",
    "\n",
    "lda_refiners = AnydoorAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_refiners = lda_refiners.load_from_safetensors(\"ckpt/anydoor_refiners_safetensors/lda.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83653863\n",
      "83653863\n"
     ]
    }
   ],
   "source": [
    "# Print nb of trainable parameters of the two models\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(lda_anydoor))\n",
    "print(count_parameters(lda_refiners))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    img = torch.randn(1,3,256,256)\n",
    "    y1 = lda_refiners.forward(img)\n",
    "    y2 = lda_anydoor.forward(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(207.3743)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(y1 - y2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from refiners.conversion.model_converter import ModelConverter\n",
    "\n",
    "converter = ModelConverter(source_model=lda_anydoor,target_model=lda_refiners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models do not have the same number of basic layers:\n",
      "  <class 'torch.nn.modules.conv.Conv2d'>: Source 72 - Target 64\n",
      "  <class 'torch.nn.modules.linear.Linear'>: Source 0 - Target 8\n",
      "Conversion failed at stage 1\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    img = torch.randn(1,3,256,256)\n",
    "    converter.run(source_args=(img,),target_args=(img,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
