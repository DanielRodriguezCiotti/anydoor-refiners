{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from refiners.fluxion.utils import manual_seed, no_grad\n",
    "from anydoor_refiners.preprocessing import preprocess_images\n",
    "from anydoor_refiners.postprocessing import post_processing\n",
    "from anydoor_refiners.model import AnyDoor\n",
    "\n",
    "\n",
    "torch.set_num_threads(2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnyDoor(device=device,dtype=dtype)\n",
    "model.unet.load_from_safetensors(\"ckpt/refiners/unet.safetensors\")\n",
    "model.control_model.load_from_safetensors(\"ckpt/refiners/controlnet.safetensors\")\n",
    "model.object_encoder.load_from_safetensors(\"ckpt/refiners/dinov2_encoder.safetensors\")\n",
    "model.lda.load_from_safetensors(\"ckpt/refiners/lda_new.safetensors\")\n",
    "True # suppress output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "uncod_scale = 5.0\n",
    "num_inference_steps = 50\n",
    "if num_inference_steps!= model.steps:\n",
    "    model.set_inference_steps(num_inference_steps, first_step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# File paths\n",
    "background_image_path = 'examples/background.png'\n",
    "background_mask_path = 'examples/background_mask.png'\n",
    "object_image_path = 'examples/object.png'\n",
    "\n",
    "# Load the object image with alpha channel\n",
    "object_image = Image.open(object_image_path).convert('RGBA')\n",
    "\n",
    "# Separate the alpha channel to create the object mask\n",
    "object_mask = (np.array(object_image.split()[-1]) > 128).astype(np.uint8)\n",
    "\n",
    "# Remove the alpha channel from the object image and convert to RGB\n",
    "object_image = object_image.convert('RGB')\n",
    "\n",
    "# Convert object_image to a NumPy array (RGB only)\n",
    "object_image = np.array(object_image)\n",
    "\n",
    "# Load the background image and background mask\n",
    "background_image = Image.open(background_image_path).convert('RGB')\n",
    "background_image = np.array(background_image).astype(np.uint8)\n",
    "\n",
    "background_mask = Image.open(background_mask_path).convert('L')\n",
    "background_mask = (np.array(background_mask) > 128).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_images = preprocess_images(np.array(object_image), np.array(object_mask), np.array(background_image.copy()), np.array(background_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# background_image_path = 'AnyDoor/examples/Gradio/BG/06.png'\n",
    "# background_mask_path = 'AnyDoor/examples/TestDreamBooth/BG/000000047948_mask.png'\n",
    "# object_image_path = 'AnyDoor/examples/TestDreamBooth/FG/01.png'\n",
    "\n",
    "# File paths\n",
    "background_image_path = 'examples/background.png'\n",
    "background_mask_path = 'examples/background_mask.png'\n",
    "object_image_path = 'examples/object.png'\n",
    "\n",
    "\n",
    "object_image = cv2.imread(object_image_path, cv2.IMREAD_UNCHANGED)\n",
    "object_mask = (object_image[:,:,-1] > 128).astype(np.uint8)\n",
    "object_image = object_image[:,:,:-1]\n",
    "object_image = cv2.cvtColor(object_image.copy(), cv2.COLOR_BGR2RGB)\n",
    "background_image = cv2.imread(background_image_path).astype(np.uint8)\n",
    "background_image = cv2.cvtColor(background_image, cv2.COLOR_BGR2RGB)\n",
    "background_mask = cv2.imread(background_mask_path)[:,:,0] > 128\n",
    "background_mask = background_mask.astype(np.uint8)\n",
    "\n",
    "preprocessed_images = preprocess_images(object_image, object_mask, background_image.copy(), background_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images):\n",
    "    fig, axs = plt.subplots(1, len(images), figsize=(15, 5))\n",
    "    for i, (title, img) in enumerate(images.items()):\n",
    "        # Rescale image if it's in the range [-1, 1] to [0, 1]\n",
    "        if np.min(img) < 0 or np.max(img) > 1:\n",
    "            img = (img + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].axis(\"off\")\n",
    "        axs[i].set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "display_images({\n",
    "    \"Object Image\": preprocessed_images[\"object\"] ,\n",
    "    \"Background Image\": preprocessed_images[\"background\"] ,\n",
    "    \"Collage Image\": preprocessed_images[\"collage\"][:,:,:-1] ,\n",
    "    \"Collage Mask\": np.stack([preprocessed_images[\"collage\"][:,:,-1]] * 3, axis=-1) ,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_tensor = torch.from_numpy(preprocessed_images['collage'].copy()).to(device=device,dtype=dtype).unsqueeze(0).permute(0,3,1,2)\n",
    "object_tensor = torch.from_numpy(preprocessed_images['object'].copy()).to(device=device ,dtype=dtype).unsqueeze(0).permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with no_grad():  \n",
    "    manual_seed(seed)\n",
    "    object_embedding = model.object_encoder.forward(object_tensor)\n",
    "    negative_object_embedding = model.object_encoder.forward(torch.zeros((1, 3, 224, 224),device=device,dtype=dtype))\n",
    "    x = model.init_latents((512, 512))\n",
    "\n",
    "    for step in tqdm(model.steps):\n",
    "        x = model.forward(\n",
    "            x,\n",
    "            step=step,\n",
    "            control_background_image= control_tensor,\n",
    "            object_embedding= object_embedding,\n",
    "            negative_object_embedding= negative_object_embedding,\n",
    "            condition_scale= 5.0\n",
    "        )\n",
    "    predicted_image = model.lda.latents_to_image(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_image.save(\"examples/predicted_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[512, 512, 512, 512]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_images[\"sizes\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "generated_image = post_processing(np.array(predicted_image),background_image,(512,512,512,512),preprocessed_images[\"background_box\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated image\n",
    "cv2.imwrite(\"examples/generated_image.png\", cv2.cvtColor(generated_image, cv2.COLOR_RGB2BGR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
